{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Make US Rich \ud83d\udcb0 Welcome, you are at the right place to learn more about the Make US Rich project. This project is a tool to help people to train, serve and use cryptocurrencies forecasting models. This is you counting money after deploying the project This project was build by @ChainYo to help people building their own MLOps projects . Warning Cryptocurrencies are only a pretext to build a machine learning project . You won't be able to use this project to make real money, but you can use it to train, serve and use your own models. In fact, you can use the project baselines to train, serve and use any kind of machine learning models . We will see how to use the project in the following sections. Please feel free to ask questions and share your ideas by opening an issue . You help and opinion is welcome in order to improve the project. Prerequisites You need to have a Python 3.8+ environment to run this project. I personally use Miniconda for this purpose. Combined with poetry , it seems to be a good way to manage dependencies and running the project in an isolated python environment. Install poetry Install Miniconda The project also requires docker and docker-compose to be installed. We will use them to run the serving and interface part of the project. To install docker and docker-compose follow the instructions on the Docker documentation . Installation Create an isolated python environment with conda : conda create -y -n make-us-rich python = 3 .8 Activate the environment and install the make-us-rich package: conda activate make-us-rich poetry add make-us-rich If you are using pip, you can use the following command instead pip install make-us-rich After everything is installed, you can run any component of the project. Architecture of the project The project is composed of three components: interface , serving and training . Each component has its own folder and its own specific configuration. All details about each component are explained in their respective documentation. Here is a simple diagram of the project: \ud83d\ude80 Let's dive into the components details! Tip You can start with any, but I recommend to start with the training component.","title":"MakeUsRich"},{"location":"#make-us-rich","text":"Welcome, you are at the right place to learn more about the Make US Rich project. This project is a tool to help people to train, serve and use cryptocurrencies forecasting models. This is you counting money after deploying the project This project was build by @ChainYo to help people building their own MLOps projects . Warning Cryptocurrencies are only a pretext to build a machine learning project . You won't be able to use this project to make real money, but you can use it to train, serve and use your own models. In fact, you can use the project baselines to train, serve and use any kind of machine learning models . We will see how to use the project in the following sections. Please feel free to ask questions and share your ideas by opening an issue . You help and opinion is welcome in order to improve the project.","title":"Make US Rich \ud83d\udcb0"},{"location":"#prerequisites","text":"You need to have a Python 3.8+ environment to run this project. I personally use Miniconda for this purpose. Combined with poetry , it seems to be a good way to manage dependencies and running the project in an isolated python environment. Install poetry Install Miniconda The project also requires docker and docker-compose to be installed. We will use them to run the serving and interface part of the project. To install docker and docker-compose follow the instructions on the Docker documentation .","title":"Prerequisites"},{"location":"#installation","text":"Create an isolated python environment with conda : conda create -y -n make-us-rich python = 3 .8 Activate the environment and install the make-us-rich package: conda activate make-us-rich poetry add make-us-rich If you are using pip, you can use the following command instead pip install make-us-rich After everything is installed, you can run any component of the project.","title":"Installation"},{"location":"#architecture-of-the-project","text":"The project is composed of three components: interface , serving and training . Each component has its own folder and its own specific configuration. All details about each component are explained in their respective documentation. Here is a simple diagram of the project: \ud83d\ude80 Let's dive into the components details! Tip You can start with any, but I recommend to start with the training component.","title":"Architecture of the project"},{"location":"interface/","text":"Incoming...","title":"Interface"},{"location":"serving/","text":"The serving component is the one that will be used to serve models and allow users to make predictions. It uses the simplicity and rapidity of FastAPI to serve the models. Setup Before we start, you need to check if you are in the right environment. If you are not, follow these instructions to setup the required environment. Once you have activated your working environment and make-us-rich is installed, we are ready to start. Initialize the serving component To initialize the serving component , run the following command: mkrich init serving This command will create a directory named mkrich-serving in your current working directory. All required files for the serving component will be created in this directory. Now navigate to the directory and open it in your favorite text editor: cd mkrich-serving # I'm using VS Code code . Don't worry if you don't use VS Code , you can open the directory in any text editor of your choice . Configure the env variables Success If you already have setup Binance API and Object Storage for the training component, you can skip this step and update the env variables files with the same credentials and values you have used for the training component. Binance API The training component requires some environment variables to be set. This component uses the Binance exchange API to fetch the required data. You can easily create an account on Binance by following the instructions and then get your API key and secret. When your binance account is created, you will be able to access the API key and secret in your account settings. Please follow these FAQ instructions to get your API key and secret. Warning Never share your API key and secret with anyone. I recommend to create an read-only API key and secret, in order to avoid any security risk. read-only means that you cannot trade with the API key and secret. Creating and validating your account could take a while. Please be patient, it's worth it. After everything is done, the access to the Binance API will be immediately available. Once you have your API key and secret, you can set them as environment variables in the dedicated env file located in /mkrich-training/conf/base/.env-binance . Info To avoid credentials exposure, a .gitignore file is created during the initialization process to exclude the conf/ folder from the git repository. Object Storage In this project we will use Minio as object storage. You can use any other object storage service, but Minio is a good choice, because it is easy to use and it is free. You can use AWS S3 or Azure as well because they are all compatible with Minio. Because running an object storage service is out of the scope of this project, we won't describe how to set up the service here and assume you already have it. Note If you don't have any object storage service, check online ressources and you will find a lot of tutorials. Finally, if you don't succeed, you still can open an issue on the make-us-rich repository to ask for help. Maybe, I will be able to help you to set up your object storage service \ud83e\udd17. When you have your object storage service ready, you can change all required env variables in the env file located in /mkrich-training/conf/base/.env-minio . ACCESS_KEY and SECRET_KEY are the credentials to access your object storage service. I recommend to create an dedicated user for this purpose, with limited permissions. The user only needs to upload and download files in the bucket you defined as BUCKET in the env file. The ENDPOINT is the URL of your object storage service used to interact with it. Launch The only thing you need to do to launch the serving component is to run the following command: mkrich run serving This command will start the serving component by building the docker image and running it. When the serving component is running, you can access it at the following URL: http://localhost:8000 . You should land on the home page which will display the endpoints of the API.","title":"Serving"},{"location":"serving/#setup","text":"Before we start, you need to check if you are in the right environment. If you are not, follow these instructions to setup the required environment. Once you have activated your working environment and make-us-rich is installed, we are ready to start.","title":"Setup"},{"location":"serving/#initialize-the-serving-component","text":"To initialize the serving component , run the following command: mkrich init serving This command will create a directory named mkrich-serving in your current working directory. All required files for the serving component will be created in this directory. Now navigate to the directory and open it in your favorite text editor: cd mkrich-serving # I'm using VS Code code . Don't worry if you don't use VS Code , you can open the directory in any text editor of your choice .","title":"Initialize the serving component"},{"location":"serving/#configure-the-env-variables","text":"Success If you already have setup Binance API and Object Storage for the training component, you can skip this step and update the env variables files with the same credentials and values you have used for the training component.","title":"Configure the env variables"},{"location":"serving/#binance-api","text":"The training component requires some environment variables to be set. This component uses the Binance exchange API to fetch the required data. You can easily create an account on Binance by following the instructions and then get your API key and secret. When your binance account is created, you will be able to access the API key and secret in your account settings. Please follow these FAQ instructions to get your API key and secret. Warning Never share your API key and secret with anyone. I recommend to create an read-only API key and secret, in order to avoid any security risk. read-only means that you cannot trade with the API key and secret. Creating and validating your account could take a while. Please be patient, it's worth it. After everything is done, the access to the Binance API will be immediately available. Once you have your API key and secret, you can set them as environment variables in the dedicated env file located in /mkrich-training/conf/base/.env-binance . Info To avoid credentials exposure, a .gitignore file is created during the initialization process to exclude the conf/ folder from the git repository.","title":"Binance API"},{"location":"serving/#object-storage","text":"In this project we will use Minio as object storage. You can use any other object storage service, but Minio is a good choice, because it is easy to use and it is free. You can use AWS S3 or Azure as well because they are all compatible with Minio. Because running an object storage service is out of the scope of this project, we won't describe how to set up the service here and assume you already have it. Note If you don't have any object storage service, check online ressources and you will find a lot of tutorials. Finally, if you don't succeed, you still can open an issue on the make-us-rich repository to ask for help. Maybe, I will be able to help you to set up your object storage service \ud83e\udd17. When you have your object storage service ready, you can change all required env variables in the env file located in /mkrich-training/conf/base/.env-minio . ACCESS_KEY and SECRET_KEY are the credentials to access your object storage service. I recommend to create an dedicated user for this purpose, with limited permissions. The user only needs to upload and download files in the bucket you defined as BUCKET in the env file. The ENDPOINT is the URL of your object storage service used to interact with it.","title":"Object Storage"},{"location":"serving/#launch","text":"The only thing you need to do to launch the serving component is to run the following command: mkrich run serving This command will start the serving component by building the docker image and running it. When the serving component is running, you can access it at the following URL: http://localhost:8000 . You should land on the home page which will display the endpoints of the API.","title":"Launch"},{"location":"training/","text":"The training component is the one that will be used to automatically schedule and launch training jobs. To automate the training process, we will use Prefect to schedule and run the training jobs and Kedro that handles the different steps represented as modular pipelines. Setup Before we start, you need to check if you are in the right environment. If you are not, follow these instructions to setup the required environment. Once you have activated your working environment and make-us-rich is installed, we are ready to start. Initialize the training component To initialize the training component , run the following command: mkrich init training This command will create a directory named mkrich-training in your current working directory. All required files for the training component will be created in this directory. Now navigate to the directory and open it in your favorite text editor: cd mkrich-training # I'm using VS Code code . Don't worry if you don't use VS Code , you can open the directory in any text editor of your choice . Configure the env variables Binance API The training component requires some environment variables to be set. This component uses the Binance exchange API to fetch the required data. You can easily create an account on Binance by following the instructions and then get your API key and secret. When your binance account is created, you will be able to access the API key and secret in your account settings. Please follow these FAQ instructions to get your API key and secret. Warning Never share your API key and secret with anyone. I recommend to create an read-only API key and secret, in order to avoid any security risk. read-only means that you cannot trade with the API key and secret. Creating and validating your account could take a while. Please be patient, it's worth it. After everything is done, the access to the Binance API will be immediately available. Once you have your API key and secret, you can set them as environment variables in the dedicated env file located in /mkrich-training/conf/base/.env-binance . Info To avoid credentials exposure, a .gitignore file is created during the initialization process to exclude the conf/ folder from the git repository. Object Storage In this project we will use Minio as object storage. You can use any other object storage service, but Minio is a good choice, because it is easy to use and it is free. You can use AWS S3 or Azure as well because they are all compatible with Minio. Because running an object storage service is out of the scope of this project, we won't describe how to set up the service here and assume you already have it. Note If you don't have any object storage service, check online ressources and you will find a lot of tutorials. Finally, if you don't succeed, you still can open an issue on the make-us-rich repository to ask for help. Maybe, I will be able to help you to set up your object storage service \ud83e\udd17. When you have your object storage service ready, you can change all required env variables in the env file located in /mkrich-training/conf/base/.env-minio . ACCESS_KEY and SECRET_KEY are the credentials to access your object storage service. I recommend to create an dedicated user for this purpose, with limited permissions. The user only needs to upload and download files in the bucket you defined as BUCKET in the env file. The ENDPOINT is the URL of your object storage service used to interact with it. Launch There is two steps to make the training component fully functional. Setup Prefect Prefect is an usefull tool to automating and monitoring the execution of tasks. We will use this great tool combined with Kedro to automate the training process. Kedro is a Python framework that allows to build modular pipelines. Because I already configured everything for you, you can launch the training component by running the following command: mkrich run training Tip Be sure to be in the mkrich-training directory before running the command. When the training component is launched, you will get access to a nice dashboard that will show you the status of registered flows . A flow is a set of tasks that will be executed in sequence by a Prefect worker, also known as an agent . By default, the dashboard is available at http://127.0.0.1:8080 . You can navigate to the flows tab to see the registered flows. If everything is working, you should see three different flows: btc_usdt , eth_usdt and chz_usdt which belongs to the make-us-rich project. All registered flows are scheduled to run every hour by default. You can change this behavior by editing the schedule of each flow. For the moment, no flow is running because Prefect needs an available agent to run the flows. So let's create a worker to run the flows. Create a local agent To create a local agent, you need to run the following command: mkrich start agent This will start a local agent that will run the registered flows in parallel in the background. If you don't want to run a local agent, you can refer to the Prefect documentation to create a remote agent or an agent in a Docker container. Stop the training component You can simply stop the agent by closing the terminal where you started it. To stop all the training component, run the following command: mkrich stop This command will stop all containers launched by Prefect and remove them. Training pipeline Steps of the training pipeline There is 5 steps for the pipeline to complete: \ud83e\ude99 Fetching data from Binance API. \ud83d\udd28 Preprocessing data: Extract features from fetched data. Split extracted features. Scale splitted features. Create sequences with scaled train features. Create sequences with scaled test features. Split train sequences as train and validation sequences. \ud83c\udfcb\ufe0f Training model. \ud83d\udd04 Converting model to ONNX format. \ud83d\udcc1 Uploading converted model to object storage service. Once all the steps are completed, you will have a trained model ready to be used and available in the object storage. Models are used by the serving component to make predictions. Training parameters You can change the training parameters by editing the dedicated file located in /mkrich-training/conf/base/parameters.yml . GPU or CPU The run_on_gpu parameter defines if the training will be done on a GPU or not. Set it to True if you have a GPU and you want to use it for training, otherwise set it to False . WandB logging The training pipeline will log the training process to WandB . You will need to create an account on WandB in order to use this feature. It's free for a personal user and you can create as many projects as you want. Once registered, you need to authenticate though the wandb cli tool. You can do it by running the following command: wandb login Don't forget to change the wandb_project parameter in the parameters.yml file if you want to use a different name for your project. Tip PyTorch Lightning has a WandB integration feature which allows us to automatically log all the training process to the WandB platform. Find help If you need help to understand or to use the training component, please open an issue on the make-us-rich repository . I will be able to help you!","title":"Training"},{"location":"training/#setup","text":"Before we start, you need to check if you are in the right environment. If you are not, follow these instructions to setup the required environment. Once you have activated your working environment and make-us-rich is installed, we are ready to start.","title":"Setup"},{"location":"training/#initialize-the-training-component","text":"To initialize the training component , run the following command: mkrich init training This command will create a directory named mkrich-training in your current working directory. All required files for the training component will be created in this directory. Now navigate to the directory and open it in your favorite text editor: cd mkrich-training # I'm using VS Code code . Don't worry if you don't use VS Code , you can open the directory in any text editor of your choice .","title":"Initialize the training component"},{"location":"training/#configure-the-env-variables","text":"","title":"Configure the env variables"},{"location":"training/#binance-api","text":"The training component requires some environment variables to be set. This component uses the Binance exchange API to fetch the required data. You can easily create an account on Binance by following the instructions and then get your API key and secret. When your binance account is created, you will be able to access the API key and secret in your account settings. Please follow these FAQ instructions to get your API key and secret. Warning Never share your API key and secret with anyone. I recommend to create an read-only API key and secret, in order to avoid any security risk. read-only means that you cannot trade with the API key and secret. Creating and validating your account could take a while. Please be patient, it's worth it. After everything is done, the access to the Binance API will be immediately available. Once you have your API key and secret, you can set them as environment variables in the dedicated env file located in /mkrich-training/conf/base/.env-binance . Info To avoid credentials exposure, a .gitignore file is created during the initialization process to exclude the conf/ folder from the git repository.","title":"Binance API"},{"location":"training/#object-storage","text":"In this project we will use Minio as object storage. You can use any other object storage service, but Minio is a good choice, because it is easy to use and it is free. You can use AWS S3 or Azure as well because they are all compatible with Minio. Because running an object storage service is out of the scope of this project, we won't describe how to set up the service here and assume you already have it. Note If you don't have any object storage service, check online ressources and you will find a lot of tutorials. Finally, if you don't succeed, you still can open an issue on the make-us-rich repository to ask for help. Maybe, I will be able to help you to set up your object storage service \ud83e\udd17. When you have your object storage service ready, you can change all required env variables in the env file located in /mkrich-training/conf/base/.env-minio . ACCESS_KEY and SECRET_KEY are the credentials to access your object storage service. I recommend to create an dedicated user for this purpose, with limited permissions. The user only needs to upload and download files in the bucket you defined as BUCKET in the env file. The ENDPOINT is the URL of your object storage service used to interact with it.","title":"Object Storage"},{"location":"training/#launch","text":"There is two steps to make the training component fully functional.","title":"Launch"},{"location":"training/#setup-prefect","text":"Prefect is an usefull tool to automating and monitoring the execution of tasks. We will use this great tool combined with Kedro to automate the training process. Kedro is a Python framework that allows to build modular pipelines. Because I already configured everything for you, you can launch the training component by running the following command: mkrich run training Tip Be sure to be in the mkrich-training directory before running the command. When the training component is launched, you will get access to a nice dashboard that will show you the status of registered flows . A flow is a set of tasks that will be executed in sequence by a Prefect worker, also known as an agent . By default, the dashboard is available at http://127.0.0.1:8080 . You can navigate to the flows tab to see the registered flows. If everything is working, you should see three different flows: btc_usdt , eth_usdt and chz_usdt which belongs to the make-us-rich project. All registered flows are scheduled to run every hour by default. You can change this behavior by editing the schedule of each flow. For the moment, no flow is running because Prefect needs an available agent to run the flows. So let's create a worker to run the flows.","title":"Setup Prefect"},{"location":"training/#create-a-local-agent","text":"To create a local agent, you need to run the following command: mkrich start agent This will start a local agent that will run the registered flows in parallel in the background. If you don't want to run a local agent, you can refer to the Prefect documentation to create a remote agent or an agent in a Docker container.","title":"Create a local agent"},{"location":"training/#stop-the-training-component","text":"You can simply stop the agent by closing the terminal where you started it. To stop all the training component, run the following command: mkrich stop This command will stop all containers launched by Prefect and remove them.","title":"Stop the training component"},{"location":"training/#training-pipeline","text":"","title":"Training pipeline"},{"location":"training/#steps-of-the-training-pipeline","text":"There is 5 steps for the pipeline to complete: \ud83e\ude99 Fetching data from Binance API. \ud83d\udd28 Preprocessing data: Extract features from fetched data. Split extracted features. Scale splitted features. Create sequences with scaled train features. Create sequences with scaled test features. Split train sequences as train and validation sequences. \ud83c\udfcb\ufe0f Training model. \ud83d\udd04 Converting model to ONNX format. \ud83d\udcc1 Uploading converted model to object storage service. Once all the steps are completed, you will have a trained model ready to be used and available in the object storage. Models are used by the serving component to make predictions.","title":"Steps of the training pipeline"},{"location":"training/#training-parameters","text":"You can change the training parameters by editing the dedicated file located in /mkrich-training/conf/base/parameters.yml .","title":"Training parameters"},{"location":"training/#gpu-or-cpu","text":"The run_on_gpu parameter defines if the training will be done on a GPU or not. Set it to True if you have a GPU and you want to use it for training, otherwise set it to False .","title":"GPU or CPU"},{"location":"training/#wandb-logging","text":"The training pipeline will log the training process to WandB . You will need to create an account on WandB in order to use this feature. It's free for a personal user and you can create as many projects as you want. Once registered, you need to authenticate though the wandb cli tool. You can do it by running the following command: wandb login Don't forget to change the wandb_project parameter in the parameters.yml file if you want to use a different name for your project. Tip PyTorch Lightning has a WandB integration feature which allows us to automatically log all the training process to the WandB platform.","title":"WandB logging"},{"location":"training/#find-help","text":"If you need help to understand or to use the training component, please open an issue on the make-us-rich repository . I will be able to help you!","title":"Find help"},{"location":"api/cli/","text":"init mkrich init [OPTIONS] SERVICE Command line interface for initializing a full project or a specific component. - serving: initialize only the serving component, constisting of an API and a web server. - interface: initialize only the interface component, constisting of a streamlit dashboard, a postgres database and a pgadmin UI. - training: initialize only the training component, constisting of a training kedro pipeline and a fully prefect ETL pipeline. Arguments: SERVICE Service to initialize ( interface, serving, training ) . [ required ] Options: -p, --path TEXT Path to initialize, defaults to current directory --help Show this message and exit. run mkrich run [OPTIONS] SERVICE Command line interface for running a specific component. You must have initialized the component before. - interface: run the streamlit dashboard. - serving: run the model serving API. - training: run the Prefect ETL component that handles the training pipeline. Arguments: SERVICE Service you want to run ( interface, serving or training ) . [ required ] Options: --help Show this message and exit. start mkrich start [OPTIONS] SERVICE Command line interface for starting a local agent that will do flows registered in the training component. - agent: start the Prefect agent. Arguments: SERVICE Service you want to start ( agent only for the moment ) . [ required ] Options: --help Show this message and exit. stop mkrich stop [OPTIONS] Command line interface for stopping all ressources deployed after ` mkrich run training ` command. Options: --help Show this message and exit.","title":"cli"},{"location":"api/cli/#init","text":"mkrich init [OPTIONS] SERVICE Command line interface for initializing a full project or a specific component. - serving: initialize only the serving component, constisting of an API and a web server. - interface: initialize only the interface component, constisting of a streamlit dashboard, a postgres database and a pgadmin UI. - training: initialize only the training component, constisting of a training kedro pipeline and a fully prefect ETL pipeline. Arguments: SERVICE Service to initialize ( interface, serving, training ) . [ required ] Options: -p, --path TEXT Path to initialize, defaults to current directory --help Show this message and exit.","title":"init"},{"location":"api/cli/#run","text":"mkrich run [OPTIONS] SERVICE Command line interface for running a specific component. You must have initialized the component before. - interface: run the streamlit dashboard. - serving: run the model serving API. - training: run the Prefect ETL component that handles the training pipeline. Arguments: SERVICE Service you want to run ( interface, serving or training ) . [ required ] Options: --help Show this message and exit.","title":"run"},{"location":"api/cli/#start","text":"mkrich start [OPTIONS] SERVICE Command line interface for starting a local agent that will do flows registered in the training component. - agent: start the Prefect agent. Arguments: SERVICE Service you want to start ( agent only for the moment ) . [ required ] Options: --help Show this message and exit.","title":"start"},{"location":"api/cli/#stop","text":"mkrich stop [OPTIONS] Command line interface for stopping all ressources deployed after ` mkrich run training ` command. Options: --help Show this message and exit.","title":"stop"},{"location":"api/client/","text":"Initializes the client for connecting to the Binance API. Source code in make_us_rich/client/binance_client.py 12 13 14 15 16 17 18 19 20 21 22 def __init__ ( self ): \"\"\" Initializes the client for connecting to the Binance API. \"\"\" try : self . _config = load_env ( \"binance\" ) except : self . _config = { \"API_KEY\" : getenv ( \"API_KEY\" ), \"SECRET_KEY\" : getenv ( \"SECRET_KEY\" )} self . client = Client ( self . _config [ \"API_KEY\" ], self . _config [ \"SECRET_KEY\" ]) self . columns = [ \"timestamp\" , \"open\" , \"high\" , \"low\" , \"close\" , \"volume\" , \"close_time\" , \"quote_av\" , \"trades\" , \"tb_base_av\" , \"tb_quote_av\" , \"ignore\" ] get_data ( symbol , interval , start_time , end_time = None ) Gets the data for the given symbol, interval, and time range. Parameters: Name Type Description Default symbol str Symbol to get the data for. required interval str Interval to get the data for. required start_time str Start time of the data. required end_time Optional [ str ] End time of the data. None Returns: Type Description pd . DataFrame Dataframe for the given symbol, interval, and time range. Source code in make_us_rich/client/binance_client.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def get_data ( self , symbol : str , interval : str , start_time : str , end_time : Optional [ str ] = None ) -> pd . DataFrame : \"\"\" Gets the data for the given symbol, interval, and time range. Parameters ---------- symbol: str Symbol to get the data for. interval: str Interval to get the data for. start_time: str Start time of the data. end_time: Optional[str] End time of the data. Returns ------- pd.DataFrame Dataframe for the given symbol, interval, and time range. \"\"\" klines = self . client . get_historical_klines ( symbol , interval , start_time , end_time ) data = pd . DataFrame ( klines , columns = self . columns ) data [ \"timestamp\" ] = pd . to_datetime ( data [ \"timestamp\" ], unit = \"ms\" ) return data get_five_days_data ( symbol ) Gets the data for the last five days. Parameters: Name Type Description Default symbol str Symbol to get the data for. required Returns: Type Description pd . DataFrame Dataframe for the last five days. Source code in make_us_rich/client/binance_client.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def get_five_days_data ( self , symbol : str ) -> pd . DataFrame : \"\"\" Gets the data for the last five days. Parameters ---------- symbol: str Symbol to get the data for. Returns ------- pd.DataFrame Dataframe for the last five days. \"\"\" symbol = symbol . upper () klines = self . client . get_historical_klines ( symbol , \"1h\" , \"5 day ago UTC\" ) data = pd . DataFrame ( klines , columns = self . columns ) data [ \"timestamp\" ] = pd . to_datetime ( data [ \"timestamp\" ], unit = \"ms\" ) return data get_one_year_data ( symbol ) Gets the data for the last year. Parameters: Name Type Description Default symbol str Symbol to get the data for. required Returns: Type Description pd . DataFrame Dataframe for the last year. Source code in make_us_rich/client/binance_client.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def get_one_year_data ( self , symbol : str ) -> pd . DataFrame : \"\"\" Gets the data for the last year. Parameters ---------- symbol: str Symbol to get the data for. Returns ------- pd.DataFrame Dataframe for the last year. \"\"\" klines = self . client . get_historical_klines ( symbol , \"1h\" , \"1 year ago UTC\" ) data = pd . DataFrame ( klines , columns = self . columns ) data [ \"timestamp\" ] = pd . to_datetime ( data [ \"timestamp\" ], unit = \"ms\" ) return data Initializes the Minio client. Returns: Type Description None Source code in make_us_rich/client/minio_client.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self ) -> None : \"\"\" Initializes the Minio client. Returns ------- None \"\"\" try : self . _config = load_env ( \"minio\" ) except : self . _config = { \"ACCESS_KEY\" : getenv ( \"ACCESS_KEY\" ), \"SECRET_KEY\" : getenv ( \"SECRET_KEY\" ), \"ENDPOINT\" : getenv ( \"ENDPOINT\" ), \"BUCKET\" : getenv ( \"BUCKET\" ) } print ( self . _config ) self . client = Minio ( endpoint = str ( self . _config [ \"ENDPOINT\" ]), access_key = self . _config [ \"ACCESS_KEY\" ], secret_key = self . _config [ \"SECRET_KEY\" ], secure = False ) self . bucket = self . _config [ \"BUCKET\" ] if not self . client . bucket_exists ( self . bucket ): self . client . make_bucket ( self . bucket ) download ( bucket , object_name , file_path ) Downloads a file from Minio. Parameters: Name Type Description Default bucket str Bucket name. required object_name str Object name. required file_path str File path. required Returns: Type Description None Source code in make_us_rich/client/minio_client.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def download ( self , bucket : str , object_name : str , file_path : str ) -> None : \"\"\" Downloads a file from Minio. Parameters ---------- bucket: str Bucket name. object_name: str Object name. file_path: str File path. Returns ------- None \"\"\" self . client . fget_object ( bucket_name = bucket , object_name = object_name , file_path = file_path ) upload ( bucket , object_name , file_path ) Uploads a file to Minio. Parameters: Name Type Description Default bucket str Bucket name. required object_name str Object name. required file_path str File path. required Returns: Type Description None Source code in make_us_rich/client/minio_client.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def upload ( self , bucket : str , object_name : str , file_path : str ) -> None : \"\"\" Uploads a file to Minio. Parameters ---------- bucket: str Bucket name. object_name: str Object name. file_path: str File path. Returns ------- None \"\"\" self . client . fput_object ( bucket_name = bucket , object_name = object_name , file_path = file_path )","title":"client"},{"location":"api/client/#make_us_rich.client.binance_client.BinanceClient.get_data","text":"Gets the data for the given symbol, interval, and time range. Parameters: Name Type Description Default symbol str Symbol to get the data for. required interval str Interval to get the data for. required start_time str Start time of the data. required end_time Optional [ str ] End time of the data. None Returns: Type Description pd . DataFrame Dataframe for the given symbol, interval, and time range. Source code in make_us_rich/client/binance_client.py 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def get_data ( self , symbol : str , interval : str , start_time : str , end_time : Optional [ str ] = None ) -> pd . DataFrame : \"\"\" Gets the data for the given symbol, interval, and time range. Parameters ---------- symbol: str Symbol to get the data for. interval: str Interval to get the data for. start_time: str Start time of the data. end_time: Optional[str] End time of the data. Returns ------- pd.DataFrame Dataframe for the given symbol, interval, and time range. \"\"\" klines = self . client . get_historical_klines ( symbol , interval , start_time , end_time ) data = pd . DataFrame ( klines , columns = self . columns ) data [ \"timestamp\" ] = pd . to_datetime ( data [ \"timestamp\" ], unit = \"ms\" ) return data","title":"get_data()"},{"location":"api/client/#make_us_rich.client.binance_client.BinanceClient.get_five_days_data","text":"Gets the data for the last five days. Parameters: Name Type Description Default symbol str Symbol to get the data for. required Returns: Type Description pd . DataFrame Dataframe for the last five days. Source code in make_us_rich/client/binance_client.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def get_five_days_data ( self , symbol : str ) -> pd . DataFrame : \"\"\" Gets the data for the last five days. Parameters ---------- symbol: str Symbol to get the data for. Returns ------- pd.DataFrame Dataframe for the last five days. \"\"\" symbol = symbol . upper () klines = self . client . get_historical_klines ( symbol , \"1h\" , \"5 day ago UTC\" ) data = pd . DataFrame ( klines , columns = self . columns ) data [ \"timestamp\" ] = pd . to_datetime ( data [ \"timestamp\" ], unit = \"ms\" ) return data","title":"get_five_days_data()"},{"location":"api/client/#make_us_rich.client.binance_client.BinanceClient.get_one_year_data","text":"Gets the data for the last year. Parameters: Name Type Description Default symbol str Symbol to get the data for. required Returns: Type Description pd . DataFrame Dataframe for the last year. Source code in make_us_rich/client/binance_client.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def get_one_year_data ( self , symbol : str ) -> pd . DataFrame : \"\"\" Gets the data for the last year. Parameters ---------- symbol: str Symbol to get the data for. Returns ------- pd.DataFrame Dataframe for the last year. \"\"\" klines = self . client . get_historical_klines ( symbol , \"1h\" , \"1 year ago UTC\" ) data = pd . DataFrame ( klines , columns = self . columns ) data [ \"timestamp\" ] = pd . to_datetime ( data [ \"timestamp\" ], unit = \"ms\" ) return data Initializes the Minio client. Returns: Type Description None Source code in make_us_rich/client/minio_client.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def __init__ ( self ) -> None : \"\"\" Initializes the Minio client. Returns ------- None \"\"\" try : self . _config = load_env ( \"minio\" ) except : self . _config = { \"ACCESS_KEY\" : getenv ( \"ACCESS_KEY\" ), \"SECRET_KEY\" : getenv ( \"SECRET_KEY\" ), \"ENDPOINT\" : getenv ( \"ENDPOINT\" ), \"BUCKET\" : getenv ( \"BUCKET\" ) } print ( self . _config ) self . client = Minio ( endpoint = str ( self . _config [ \"ENDPOINT\" ]), access_key = self . _config [ \"ACCESS_KEY\" ], secret_key = self . _config [ \"SECRET_KEY\" ], secure = False ) self . bucket = self . _config [ \"BUCKET\" ] if not self . client . bucket_exists ( self . bucket ): self . client . make_bucket ( self . bucket )","title":"get_one_year_data()"},{"location":"api/client/#make_us_rich.client.minio_client.MinioClient.download","text":"Downloads a file from Minio. Parameters: Name Type Description Default bucket str Bucket name. required object_name str Object name. required file_path str File path. required Returns: Type Description None Source code in make_us_rich/client/minio_client.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def download ( self , bucket : str , object_name : str , file_path : str ) -> None : \"\"\" Downloads a file from Minio. Parameters ---------- bucket: str Bucket name. object_name: str Object name. file_path: str File path. Returns ------- None \"\"\" self . client . fget_object ( bucket_name = bucket , object_name = object_name , file_path = file_path )","title":"download()"},{"location":"api/client/#make_us_rich.client.minio_client.MinioClient.upload","text":"Uploads a file to Minio. Parameters: Name Type Description Default bucket str Bucket name. required object_name str Object name. required file_path str File path. required Returns: Type Description None Source code in make_us_rich/client/minio_client.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def upload ( self , bucket : str , object_name : str , file_path : str ) -> None : \"\"\" Uploads a file to Minio. Parameters ---------- bucket: str Bucket name. object_name: str Object name. file_path: str File path. Returns ------- None \"\"\" self . client . fput_object ( bucket_name = bucket , object_name = object_name , file_path = file_path )","title":"upload()"},{"location":"api/interface/","text":"ApiRequest Class that handles the API requests for the interface Source code in make_us_rich/interface/api_request.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ApiRequest : \"\"\" Class that handles the API requests for the interface \"\"\" def __init__ ( self ): try : self . _config = load_env ( \"api\" ) except : self . _config = { \"URL\" : getenv ( \"URL\" )} self . url = self . _config [ \"URL\" ] def prediction ( self , currency : str , compare : str , token : str ) -> Dict : \"\"\" Predict endpoint. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. token: str API token of the user. Returns ------- Dict Dictionary containing the data and the prediction. \"\"\" return requests . put ( f \" { self . url } /predict\" , params = { \"currency\" : currency , \"compare\" : compare , \"token\" : token } ) . json () def number_of_available_models ( self ) -> Dict : \"\"\" Number of available models endpoint. Returns ------- Dict Dictionary containing the number of available models. \"\"\" return requests . get ( f \" { self . url } /check_models_number\" ) . json () number_of_available_models () Number of available models endpoint. Returns: Type Description Dict Dictionary containing the number of available models. Source code in make_us_rich/interface/api_request.py 45 46 47 48 49 50 51 52 53 54 def number_of_available_models ( self ) -> Dict : \"\"\" Number of available models endpoint. Returns ------- Dict Dictionary containing the number of available models. \"\"\" return requests . get ( f \" { self . url } /check_models_number\" ) . json () prediction ( currency , compare , token ) Predict endpoint. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required token str API token of the user. required Returns: Type Description Dict Dictionary containing the data and the prediction. Source code in make_us_rich/interface/api_request.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def prediction ( self , currency : str , compare : str , token : str ) -> Dict : \"\"\" Predict endpoint. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. token: str API token of the user. Returns ------- Dict Dictionary containing the data and the prediction. \"\"\" return requests . put ( f \" { self . url } /predict\" , params = { \"currency\" : currency , \"compare\" : compare , \"token\" : token } ) . json () Authentication Source code in make_us_rich/interface/authentication.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Authentication : def __init__ ( self , key : str = \"mur-key\" , cookie_ttl : int = 10 ) -> Tuple [ str , str , bool ]: \"\"\" Initialize the Authentication class. Parameters ---------- cookie_ttl: int The time to live of the cookie. Default is 10. Returns ------- Tuple[str, bool]: The token and the status of the authentication. \"\"\" self . key = key self . cookie_ttl = cookie_ttl self . cookie_name = \"make_us_rich_auth\" def _generate_cookie_token ( self ) -> str : \"\"\" Generate a random string to be used as the token for authentication cookie. Returns ------- str: The JWT token. \"\"\" return jwt . encode ( payload = { \"username\" : st . session_state [ \"username\" ], \"expiration_date\" : self . _generate_expiration_date () }, key = self . key , ) def _generate_expiration_date ( self ) -> str : \"\"\" Generate the expiration date of the token. Returns ------- str: The expiration date of the token. \"\"\" return ( datetime . utcnow () + timedelta ( days = self . cookie_ttl )) . timestamp () def _decode_token ( self ) -> str : \"\"\" Decode the token. Returns ------- str: The decoded token. \"\"\" return jwt . decode ( self . token , key = self . key , algorithms = [ \"HS256\" ]) def login ( self , form_title : str ) -> Tuple [ str , str , bool ]: \"\"\"\"\"\" self . form_title = form_title cookie_manager = stx . CookieManager () if \"authentication_status\" not in st . session_state : st . session_state [ \"authentication_status\" ] = None if \"username\" not in st . session_state : st . session_state [ \"username\" ] = None if \"role\" not in st . session_state : st . session_state [ \"role\" ] = None if \"api_token\" not in st . session_state : st . session_state [ \"api_token\" ] = None if st . session_state [ \"authentication_status\" ] != True : try : self . token = cookie_manager . get ( self . cookie_name ) self . token = self . _decode_token () if self . token [ \"expiration_date\" ] > datetime . utcnow () . timestamp (): st . session_state [ \"authentication_status\" ] = True st . session_state [ \"username\" ] = self . token [ \"username\" ] else : st . session_state [ \"authentication_status\" ] = False except : st . session_state [ \"authentication_status\" ] = None if st . session_state [ \"authentication_status\" ] != True : login_form = st . form ( \"Login\" ) login_form . subheader ( \"Welcome \ud83d\udc4b, please login first. \\n \" \"If you don't have an account, it will be created automatically when you submit the form.\" ) input_username_value = st . session_state [ \"username\" ] if st . session_state [ \"username\" ] else \"\" self . username = login_form . text_input ( \"Username\" , value = input_username_value ) self . password = login_form . text_input ( \"Password\" , type = \"password\" ) if login_form . form_submit_button ( \"Submit\" ): user_exist = DatabaseHandler . check_if_user_exist ( self . username ) if user_exist [ \"success\" ]: results = DatabaseHandler . authentication ( self . username , self . password ) login_message = f \" { results [ 'message' ] } Welcome back { self . username } ! \ud83c\udf89\" else : results = DatabaseHandler . create_user ( self . username , self . password ) login_message = f \" { results [ 'message' ] } Welcome { self . username } ! \ud83c\udf89\" if results [ \"success\" ]: st . session_state [ \"authentication_status\" ] = True st . session_state [ \"username\" ] = results [ \"username\" ] self . token = self . _generate_cookie_token () cookie_manager . set ( self . cookie_name , self . token , expires_at = datetime . now () + timedelta ( self . cookie_ttl ) ) st . success ( login_message ) time . sleep ( 5 ) else : st . error ( results [ \"message\" ]) if st . session_state [ \"authentication_status\" ] == True : st . session_state [ \"role\" ] = DatabaseHandler . _check_user_role ( st . session_state [ \"username\" ])[ \"role\" ] st . session_state [ \"api_token\" ] = DatabaseHandler . get_api_token ( st . session_state [ \"username\" ])[ \"token\" ] st . sidebar . title ( \"User Panel\" ) st . sidebar . markdown ( f \"** { st . session_state [ 'username' ] } **, log out by clicking the button below.\" , unsafe_allow_html = True ) if st . sidebar . button ( \"Logout\" , key = \"logout\" ): cookie_manager . delete ( self . cookie_name ) st . session_state [ \"authentication_status\" ] = None st . session_state [ \"username\" ] = None st . session_state [ \"role\" ] = None return ( st . session_state [ \"username\" ], st . session_state [ \"role\" ], st . session_state [ \"api_token\" ], st . session_state [ \"authentication_status\" ], ) __init__ ( key = 'mur-key' , cookie_ttl = 10 ) Initialize the Authentication class. Parameters: Name Type Description Default cookie_ttl int The time to live of the cookie. Default is 10. 10 Returns: Type Description Tuple [ str , bool ] The token and the status of the authentication. Source code in make_us_rich/interface/authentication.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , key : str = \"mur-key\" , cookie_ttl : int = 10 ) -> Tuple [ str , str , bool ]: \"\"\" Initialize the Authentication class. Parameters ---------- cookie_ttl: int The time to live of the cookie. Default is 10. Returns ------- Tuple[str, bool]: The token and the status of the authentication. \"\"\" self . key = key self . cookie_ttl = cookie_ttl self . cookie_name = \"make_us_rich_auth\" _decode_token () Decode the token. Returns: Name Type Description str str The decoded token. Source code in make_us_rich/interface/authentication.py 62 63 64 65 66 67 68 69 70 71 def _decode_token ( self ) -> str : \"\"\" Decode the token. Returns ------- str: The decoded token. \"\"\" return jwt . decode ( self . token , key = self . key , algorithms = [ \"HS256\" ]) _generate_cookie_token () Generate a random string to be used as the token for authentication cookie. Returns: Name Type Description str str The JWT token. Source code in make_us_rich/interface/authentication.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def _generate_cookie_token ( self ) -> str : \"\"\" Generate a random string to be used as the token for authentication cookie. Returns ------- str: The JWT token. \"\"\" return jwt . encode ( payload = { \"username\" : st . session_state [ \"username\" ], \"expiration_date\" : self . _generate_expiration_date () }, key = self . key , ) _generate_expiration_date () Generate the expiration date of the token. Returns: Name Type Description str str The expiration date of the token. Source code in make_us_rich/interface/authentication.py 50 51 52 53 54 55 56 57 58 59 def _generate_expiration_date ( self ) -> str : \"\"\" Generate the expiration date of the token. Returns ------- str: The expiration date of the token. \"\"\" return ( datetime . utcnow () + timedelta ( days = self . cookie_ttl )) . timestamp () DatabaseHandler This class handles the database connection and the queries. Source code in make_us_rich/interface/database_handler.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 class DatabaseHandler : \"\"\" This class handles the database connection and the queries. \"\"\" @classmethod def authentication ( cls , username : str , password : str ) -> Dict [ str , bool ]: \"\"\" Checks if the user and password are correct Parameters ---------- username: str The username of the user. password: str The password of the user. Returns ------- dict: A dictionary with the success of the authentication. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT id FROM users WHERE username = ' { username } ' AND password = crypt(' { password } ', password); \"\"\" ) match = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"message\" : \"Authentication successful.\" , \"username\" : username } if match else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} @classmethod def create_user ( cls , username : str , password : str ) -> Dict [ str , Any ]: \"\"\" Creates a new user in the database. Parameters ---------- username: str The username of the new user. password: str The password of the new user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO users (username, password) VALUES (' { username } ', crypt(' { password } ', gen_salt('bf'))) ON CONFLICT (username) DO NOTHING; \"\"\" ) cls . connection . commit () cls . _disconnect () cls . _add_member_role_to_user ( username ) cls . _generate_token_for_user ( username ) cls . _init_api_limit_for_user ( username ) return { \"success\" : True , \"message\" : \"User created successfully.\" , \"username\" : username , } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} @classmethod def check_if_user_exist ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Checks if a user exists in the database. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the check. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT id FROM users WHERE username = ' { username } '; \"\"\" ) match = cursor . fetchone () cls . _disconnect () return { \"success\" : True } if match else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} @classmethod def get_api_token ( cls , username : str ) -> Dict [ str , str ]: \"\"\" Gets the API token of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the API token of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT token FROM api_tokens JOIN users ON users.id = api_tokens.user_id WHERE users.username = ' { username } '; \"\"\" ) token = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"token\" : token [ 0 ]} if token else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} @classmethod def increment_user_api_consumption ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Increments the API consumption of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the increment. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" UPDATE user_api_consumptions SET api_consumption = api_consumption + 1 WHERE user_id = (SELECT id FROM users WHERE username = ' { username } '); \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"error\" : str ( e )} @classmethod def get_user_api_consumption ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Gets the API consumption of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the API consumption of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT api_consumption FROM user_api_consumptions WHERE user_id = (SELECT id FROM users WHERE username = ' { username } '); \"\"\" ) consumption = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"consumption\" : consumption [ 0 ]} except Exception as e : return { \"error\" : str ( e )} @classmethod def _check_user_role ( cls , username : str ) -> Dict [ str , str ]: \"\"\" Checks the role of the user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the role of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT name FROM user_roles JOIN users ON users.id = user_roles.user_id JOIN roles ON roles.id = user_roles.role_id WHERE users.username = ' { username } '; \"\"\" ) role = cursor . fetchone ()[ 0 ] cls . _disconnect () return { \"role\" : role } except Exception as e : return { \"error\" : str ( e )} @classmethod def _add_member_role_to_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Adds the role of the user to the database. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO user_roles (user_id, role_id) SELECT u.id, r.id FROM (SELECT id FROM users WHERE username = ' { username } ') u, (SELECT id FROM roles WHERE name = 'member') r; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} @classmethod def _generate_token_for_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Generates a token for the user. Parameters ---------- username: str The username of the user. Returns ------- str: The token of the user. \"\"\" token = random_string () try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO api_tokens (user_id, token) SELECT u.id, ' { token } ' FROM (SELECT id FROM users WHERE username = ' { username } ') u; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} @classmethod def _init_api_limit_for_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Initializes the API limit for the user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO user_api_consumptions (user_id, api_consumption) SELECT u.id, 0 FROM (SELECT id FROM users WHERE username = ' { username } ') u; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} @classmethod def _connect ( cls ): \"\"\" Connects to the database. \"\"\" try : cls . connection = psycopg2 . connect ( database = getenv ( \"POSTGRES_DB\" ), user = getenv ( \"POSTGRES_USER\" ), host = getenv ( \"HOST\" ), password = getenv ( \"POSTGRES_PASSWORD\" ) ) except Exception as e : return e @classmethod def _disconnect ( cls ): \"\"\" Closes the connection to the database. \"\"\" try : cls . connection . close () except Exception as e : return e _add_member_role_to_user ( username ) classmethod Adds the role of the user to the database. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the creation. Source code in make_us_rich/interface/database_handler.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 @classmethod def _add_member_role_to_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Adds the role of the user to the database. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO user_roles (user_id, role_id) SELECT u.id, r.id FROM (SELECT id FROM users WHERE username = ' { username } ') u, (SELECT id FROM roles WHERE name = 'member') r; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} _check_user_role ( username ) classmethod Checks the role of the user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , str ] A dictionary with the role of the user. Source code in make_us_rich/interface/database_handler.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @classmethod def _check_user_role ( cls , username : str ) -> Dict [ str , str ]: \"\"\" Checks the role of the user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the role of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT name FROM user_roles JOIN users ON users.id = user_roles.user_id JOIN roles ON roles.id = user_roles.role_id WHERE users.username = ' { username } '; \"\"\" ) role = cursor . fetchone ()[ 0 ] cls . _disconnect () return { \"role\" : role } except Exception as e : return { \"error\" : str ( e )} _connect () classmethod Connects to the database. Source code in make_us_rich/interface/database_handler.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 @classmethod def _connect ( cls ): \"\"\" Connects to the database. \"\"\" try : cls . connection = psycopg2 . connect ( database = getenv ( \"POSTGRES_DB\" ), user = getenv ( \"POSTGRES_USER\" ), host = getenv ( \"HOST\" ), password = getenv ( \"POSTGRES_PASSWORD\" ) ) except Exception as e : return e _disconnect () classmethod Closes the connection to the database. Source code in make_us_rich/interface/database_handler.py 338 339 340 341 342 343 344 345 346 @classmethod def _disconnect ( cls ): \"\"\" Closes the connection to the database. \"\"\" try : cls . connection . close () except Exception as e : return e _generate_token_for_user ( username ) classmethod Generates a token for the user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description str Dict [ str , Any ] The token of the user. Source code in make_us_rich/interface/database_handler.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 @classmethod def _generate_token_for_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Generates a token for the user. Parameters ---------- username: str The username of the user. Returns ------- str: The token of the user. \"\"\" token = random_string () try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO api_tokens (user_id, token) SELECT u.id, ' { token } ' FROM (SELECT id FROM users WHERE username = ' { username } ') u; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} _init_api_limit_for_user ( username ) classmethod Initializes the API limit for the user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the creation. Source code in make_us_rich/interface/database_handler.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @classmethod def _init_api_limit_for_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Initializes the API limit for the user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO user_api_consumptions (user_id, api_consumption) SELECT u.id, 0 FROM (SELECT id FROM users WHERE username = ' { username } ') u; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} authentication ( username , password ) classmethod Checks if the user and password are correct Parameters: Name Type Description Default username str The username of the user. required password str The password of the user. required Returns: Name Type Description dict Dict [ str , bool ] A dictionary with the success of the authentication. Source code in make_us_rich/interface/database_handler.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @classmethod def authentication ( cls , username : str , password : str ) -> Dict [ str , bool ]: \"\"\" Checks if the user and password are correct Parameters ---------- username: str The username of the user. password: str The password of the user. Returns ------- dict: A dictionary with the success of the authentication. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT id FROM users WHERE username = ' { username } ' AND password = crypt(' { password } ', password); \"\"\" ) match = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"message\" : \"Authentication successful.\" , \"username\" : username } if match else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} check_if_user_exist ( username ) classmethod Checks if a user exists in the database. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the check. Source code in make_us_rich/interface/database_handler.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @classmethod def check_if_user_exist ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Checks if a user exists in the database. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the check. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT id FROM users WHERE username = ' { username } '; \"\"\" ) match = cursor . fetchone () cls . _disconnect () return { \"success\" : True } if match else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} create_user ( username , password ) classmethod Creates a new user in the database. Parameters: Name Type Description Default username str The username of the new user. required password str The password of the new user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the creation. Source code in make_us_rich/interface/database_handler.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @classmethod def create_user ( cls , username : str , password : str ) -> Dict [ str , Any ]: \"\"\" Creates a new user in the database. Parameters ---------- username: str The username of the new user. password: str The password of the new user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO users (username, password) VALUES (' { username } ', crypt(' { password } ', gen_salt('bf'))) ON CONFLICT (username) DO NOTHING; \"\"\" ) cls . connection . commit () cls . _disconnect () cls . _add_member_role_to_user ( username ) cls . _generate_token_for_user ( username ) cls . _init_api_limit_for_user ( username ) return { \"success\" : True , \"message\" : \"User created successfully.\" , \"username\" : username , } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} get_api_token ( username ) classmethod Gets the API token of a user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , str ] A dictionary with the API token of the user. Source code in make_us_rich/interface/database_handler.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @classmethod def get_api_token ( cls , username : str ) -> Dict [ str , str ]: \"\"\" Gets the API token of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the API token of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT token FROM api_tokens JOIN users ON users.id = api_tokens.user_id WHERE users.username = ' { username } '; \"\"\" ) token = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"token\" : token [ 0 ]} if token else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} get_user_api_consumption ( username ) classmethod Gets the API consumption of a user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the API consumption of the user. Source code in make_us_rich/interface/database_handler.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 @classmethod def get_user_api_consumption ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Gets the API consumption of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the API consumption of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT api_consumption FROM user_api_consumptions WHERE user_id = (SELECT id FROM users WHERE username = ' { username } '); \"\"\" ) consumption = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"consumption\" : consumption [ 0 ]} except Exception as e : return { \"error\" : str ( e )} increment_user_api_consumption ( username ) classmethod Increments the API consumption of a user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the increment. Source code in make_us_rich/interface/database_handler.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 @classmethod def increment_user_api_consumption ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Increments the API consumption of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the increment. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" UPDATE user_api_consumptions SET api_consumption = api_consumption + 1 WHERE user_id = (SELECT id FROM users WHERE username = ' { username } '); \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"error\" : str ( e )} Plots candlestick_plot ( data , currency , compare , pred ) Create candlestick plot. Parameters: Name Type Description Default data pd . DataFrame Dataframe containing the data to plot. required currency str Currency to plot. required compare str Currency to compare. required pred float Prediction to plot. required Returns: Type Description go . Figure Plotly figure. Source code in make_us_rich/interface/plots.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def candlestick_plot ( data : pd . DataFrame , currency : str , compare : str , pred : float ) -> go . Figure : \"\"\" Create candlestick plot. Parameters ---------- data : pd.DataFrame Dataframe containing the data to plot. currency : str Currency to plot. compare : str Currency to compare. pred : float Prediction to plot. Returns ------- go.Figure Plotly figure. \"\"\" fig = go . Figure () fig . add_trace ( go . Candlestick ( x = data [ \"timestamp\" ], open = data [ \"open\" ], high = data [ \"high\" ], low = data [ \"low\" ], close = data [ \"close\" ], name = \"Candlestick\" ) ) fig . update_layout ( title = f \" { currency . upper () } / { compare . upper () } - last 5 days\" , yaxis_title = f \" { currency . upper () } Price\" , ) pred = float ( data . iloc [ - 1 ][ \"close\" ]) + pred timestamp = data [ \"timestamp\" ] . max () + timedelta ( hours = 1 ) fig . add_trace ( go . Scatter ( x = [ timestamp ], y = [ pred ], mode = \"markers\" , marker_color = \"red\" , name = \"Prediction\" ) ) fig . update_layout ( title = f \" { currency . upper () } / { compare . upper () } - last 5 days\" , yaxis_title = f \" { currency . upper () } Price\" , annotations = [ go . Annotation ( x = timestamp , y = pred , text = f \"Prediction: { pred : .3f } \" , showarrow = False , ) ], ) return fig format_data ( data ) Format data from API response before plotting. Parameters: Name Type Description Default data Dict [ str , Any ] Data from API response. required Returns: Type Description Tuple [ pd . DataFrame , float ] Dataframe containing the data to plot and the prediction. Source code in make_us_rich/interface/plots.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def format_data ( data : Dict [ str , Any ]) -> Tuple [ pd . DataFrame , float ]: \"\"\" Format data from API response before plotting. Parameters ---------- data : Dict[str, Any] Data from API response. Returns ------- Tuple[pd.DataFrame, float] Dataframe containing the data to plot and the prediction. \"\"\" pred = data [ \"prediction\" ] df = pd . DataFrame ( data = data [ \"data\" ], columns = [ \"timestamp\" , \"open\" , \"high\" , \"low\" , \"close\" , \"volume\" , \"close_time\" , \"quote_av\" , \"trades\" , \"tb_base_av\" , \"tb_quote_av\" , \"ignore\" , ], ) df [ \"timestamp\" ] = pd . to_datetime ( df [ \"timestamp\" ]) df [ \"close\" ] = pd . to_numeric ( df [ \"close\" ], downcast = \"float\" ) df [ \"open\" ] = pd . to_numeric ( df [ \"open\" ], downcast = \"float\" ) df [ \"high\" ] = pd . to_numeric ( df [ \"high\" ], downcast = \"float\" ) df [ \"low\" ] = pd . to_numeric ( df [ \"low\" ], downcast = \"float\" ) return df , pred scatter_plot ( data , currency , compare , pred ) Create scatter plot. Parameters: Name Type Description Default data pd . DataFrame Dataframe containing the data to plot. required currency str Currency to plot. required compare str Currency to compare. required pred float Prediction to plot. required Returns: Type Description go . Figure Plotly figure. Source code in make_us_rich/interface/plots.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def scatter_plot ( data : pd . DataFrame , currency : str , compare : str , pred : float ) -> go . Figure : \"\"\" Create scatter plot. Parameters ---------- data : pd.DataFrame Dataframe containing the data to plot. currency : str Currency to plot. compare : str Currency to compare. pred : float Prediction to plot. Returns ------- go.Figure Plotly figure. \"\"\" fig = go . Figure () fig . add_trace ( go . Scatter ( x = data [ \"timestamp\" ], y = data [ \"close\" ], name = \"Close Price\" , line_color = \"blue\" , connectgaps = True ) ) fig . update_layout ( title = f \" { currency . upper () } / { compare . upper () } - last 5 days\" , yaxis_title = f \" { currency . upper () } Price\" , ) pred = float ( data . iloc [ - 1 ][ \"close\" ]) + pred timestamp = data [ \"timestamp\" ] . max () + timedelta ( hours = 1 ) fig . add_trace ( go . Scatter ( x = [ timestamp ], y = [ pred ], mode = \"markers\" , marker_color = \"red\" , name = \"Prediction\" ) ) fig . update_layout ( title = f \" { currency . upper () } / { compare . upper () } - last 5 days\" , yaxis_title = f \" { currency . upper () } Price\" , annotations = [ go . Annotation ( x = timestamp , y = pred , text = f \"Prediction: { pred : .3f } \" , showarrow = False , ) ], ) return fig","title":"interface"},{"location":"api/interface/#apirequest","text":"Class that handles the API requests for the interface Source code in make_us_rich/interface/api_request.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 class ApiRequest : \"\"\" Class that handles the API requests for the interface \"\"\" def __init__ ( self ): try : self . _config = load_env ( \"api\" ) except : self . _config = { \"URL\" : getenv ( \"URL\" )} self . url = self . _config [ \"URL\" ] def prediction ( self , currency : str , compare : str , token : str ) -> Dict : \"\"\" Predict endpoint. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. token: str API token of the user. Returns ------- Dict Dictionary containing the data and the prediction. \"\"\" return requests . put ( f \" { self . url } /predict\" , params = { \"currency\" : currency , \"compare\" : compare , \"token\" : token } ) . json () def number_of_available_models ( self ) -> Dict : \"\"\" Number of available models endpoint. Returns ------- Dict Dictionary containing the number of available models. \"\"\" return requests . get ( f \" { self . url } /check_models_number\" ) . json ()","title":"ApiRequest"},{"location":"api/interface/#make_us_rich.interface.api_request.ApiRequest.number_of_available_models","text":"Number of available models endpoint. Returns: Type Description Dict Dictionary containing the number of available models. Source code in make_us_rich/interface/api_request.py 45 46 47 48 49 50 51 52 53 54 def number_of_available_models ( self ) -> Dict : \"\"\" Number of available models endpoint. Returns ------- Dict Dictionary containing the number of available models. \"\"\" return requests . get ( f \" { self . url } /check_models_number\" ) . json ()","title":"number_of_available_models()"},{"location":"api/interface/#make_us_rich.interface.api_request.ApiRequest.prediction","text":"Predict endpoint. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required token str API token of the user. required Returns: Type Description Dict Dictionary containing the data and the prediction. Source code in make_us_rich/interface/api_request.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def prediction ( self , currency : str , compare : str , token : str ) -> Dict : \"\"\" Predict endpoint. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. token: str API token of the user. Returns ------- Dict Dictionary containing the data and the prediction. \"\"\" return requests . put ( f \" { self . url } /predict\" , params = { \"currency\" : currency , \"compare\" : compare , \"token\" : token } ) . json ()","title":"prediction()"},{"location":"api/interface/#authentication","text":"Source code in make_us_rich/interface/authentication.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Authentication : def __init__ ( self , key : str = \"mur-key\" , cookie_ttl : int = 10 ) -> Tuple [ str , str , bool ]: \"\"\" Initialize the Authentication class. Parameters ---------- cookie_ttl: int The time to live of the cookie. Default is 10. Returns ------- Tuple[str, bool]: The token and the status of the authentication. \"\"\" self . key = key self . cookie_ttl = cookie_ttl self . cookie_name = \"make_us_rich_auth\" def _generate_cookie_token ( self ) -> str : \"\"\" Generate a random string to be used as the token for authentication cookie. Returns ------- str: The JWT token. \"\"\" return jwt . encode ( payload = { \"username\" : st . session_state [ \"username\" ], \"expiration_date\" : self . _generate_expiration_date () }, key = self . key , ) def _generate_expiration_date ( self ) -> str : \"\"\" Generate the expiration date of the token. Returns ------- str: The expiration date of the token. \"\"\" return ( datetime . utcnow () + timedelta ( days = self . cookie_ttl )) . timestamp () def _decode_token ( self ) -> str : \"\"\" Decode the token. Returns ------- str: The decoded token. \"\"\" return jwt . decode ( self . token , key = self . key , algorithms = [ \"HS256\" ]) def login ( self , form_title : str ) -> Tuple [ str , str , bool ]: \"\"\"\"\"\" self . form_title = form_title cookie_manager = stx . CookieManager () if \"authentication_status\" not in st . session_state : st . session_state [ \"authentication_status\" ] = None if \"username\" not in st . session_state : st . session_state [ \"username\" ] = None if \"role\" not in st . session_state : st . session_state [ \"role\" ] = None if \"api_token\" not in st . session_state : st . session_state [ \"api_token\" ] = None if st . session_state [ \"authentication_status\" ] != True : try : self . token = cookie_manager . get ( self . cookie_name ) self . token = self . _decode_token () if self . token [ \"expiration_date\" ] > datetime . utcnow () . timestamp (): st . session_state [ \"authentication_status\" ] = True st . session_state [ \"username\" ] = self . token [ \"username\" ] else : st . session_state [ \"authentication_status\" ] = False except : st . session_state [ \"authentication_status\" ] = None if st . session_state [ \"authentication_status\" ] != True : login_form = st . form ( \"Login\" ) login_form . subheader ( \"Welcome \ud83d\udc4b, please login first. \\n \" \"If you don't have an account, it will be created automatically when you submit the form.\" ) input_username_value = st . session_state [ \"username\" ] if st . session_state [ \"username\" ] else \"\" self . username = login_form . text_input ( \"Username\" , value = input_username_value ) self . password = login_form . text_input ( \"Password\" , type = \"password\" ) if login_form . form_submit_button ( \"Submit\" ): user_exist = DatabaseHandler . check_if_user_exist ( self . username ) if user_exist [ \"success\" ]: results = DatabaseHandler . authentication ( self . username , self . password ) login_message = f \" { results [ 'message' ] } Welcome back { self . username } ! \ud83c\udf89\" else : results = DatabaseHandler . create_user ( self . username , self . password ) login_message = f \" { results [ 'message' ] } Welcome { self . username } ! \ud83c\udf89\" if results [ \"success\" ]: st . session_state [ \"authentication_status\" ] = True st . session_state [ \"username\" ] = results [ \"username\" ] self . token = self . _generate_cookie_token () cookie_manager . set ( self . cookie_name , self . token , expires_at = datetime . now () + timedelta ( self . cookie_ttl ) ) st . success ( login_message ) time . sleep ( 5 ) else : st . error ( results [ \"message\" ]) if st . session_state [ \"authentication_status\" ] == True : st . session_state [ \"role\" ] = DatabaseHandler . _check_user_role ( st . session_state [ \"username\" ])[ \"role\" ] st . session_state [ \"api_token\" ] = DatabaseHandler . get_api_token ( st . session_state [ \"username\" ])[ \"token\" ] st . sidebar . title ( \"User Panel\" ) st . sidebar . markdown ( f \"** { st . session_state [ 'username' ] } **, log out by clicking the button below.\" , unsafe_allow_html = True ) if st . sidebar . button ( \"Logout\" , key = \"logout\" ): cookie_manager . delete ( self . cookie_name ) st . session_state [ \"authentication_status\" ] = None st . session_state [ \"username\" ] = None st . session_state [ \"role\" ] = None return ( st . session_state [ \"username\" ], st . session_state [ \"role\" ], st . session_state [ \"api_token\" ], st . session_state [ \"authentication_status\" ], )","title":"Authentication"},{"location":"api/interface/#make_us_rich.interface.authentication.Authentication.__init__","text":"Initialize the Authentication class. Parameters: Name Type Description Default cookie_ttl int The time to live of the cookie. Default is 10. 10 Returns: Type Description Tuple [ str , bool ] The token and the status of the authentication. Source code in make_us_rich/interface/authentication.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , key : str = \"mur-key\" , cookie_ttl : int = 10 ) -> Tuple [ str , str , bool ]: \"\"\" Initialize the Authentication class. Parameters ---------- cookie_ttl: int The time to live of the cookie. Default is 10. Returns ------- Tuple[str, bool]: The token and the status of the authentication. \"\"\" self . key = key self . cookie_ttl = cookie_ttl self . cookie_name = \"make_us_rich_auth\"","title":"__init__()"},{"location":"api/interface/#make_us_rich.interface.authentication.Authentication._decode_token","text":"Decode the token. Returns: Name Type Description str str The decoded token. Source code in make_us_rich/interface/authentication.py 62 63 64 65 66 67 68 69 70 71 def _decode_token ( self ) -> str : \"\"\" Decode the token. Returns ------- str: The decoded token. \"\"\" return jwt . decode ( self . token , key = self . key , algorithms = [ \"HS256\" ])","title":"_decode_token()"},{"location":"api/interface/#make_us_rich.interface.authentication.Authentication._generate_cookie_token","text":"Generate a random string to be used as the token for authentication cookie. Returns: Name Type Description str str The JWT token. Source code in make_us_rich/interface/authentication.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def _generate_cookie_token ( self ) -> str : \"\"\" Generate a random string to be used as the token for authentication cookie. Returns ------- str: The JWT token. \"\"\" return jwt . encode ( payload = { \"username\" : st . session_state [ \"username\" ], \"expiration_date\" : self . _generate_expiration_date () }, key = self . key , )","title":"_generate_cookie_token()"},{"location":"api/interface/#make_us_rich.interface.authentication.Authentication._generate_expiration_date","text":"Generate the expiration date of the token. Returns: Name Type Description str str The expiration date of the token. Source code in make_us_rich/interface/authentication.py 50 51 52 53 54 55 56 57 58 59 def _generate_expiration_date ( self ) -> str : \"\"\" Generate the expiration date of the token. Returns ------- str: The expiration date of the token. \"\"\" return ( datetime . utcnow () + timedelta ( days = self . cookie_ttl )) . timestamp ()","title":"_generate_expiration_date()"},{"location":"api/interface/#databasehandler","text":"This class handles the database connection and the queries. Source code in make_us_rich/interface/database_handler.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 class DatabaseHandler : \"\"\" This class handles the database connection and the queries. \"\"\" @classmethod def authentication ( cls , username : str , password : str ) -> Dict [ str , bool ]: \"\"\" Checks if the user and password are correct Parameters ---------- username: str The username of the user. password: str The password of the user. Returns ------- dict: A dictionary with the success of the authentication. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT id FROM users WHERE username = ' { username } ' AND password = crypt(' { password } ', password); \"\"\" ) match = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"message\" : \"Authentication successful.\" , \"username\" : username } if match else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} @classmethod def create_user ( cls , username : str , password : str ) -> Dict [ str , Any ]: \"\"\" Creates a new user in the database. Parameters ---------- username: str The username of the new user. password: str The password of the new user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO users (username, password) VALUES (' { username } ', crypt(' { password } ', gen_salt('bf'))) ON CONFLICT (username) DO NOTHING; \"\"\" ) cls . connection . commit () cls . _disconnect () cls . _add_member_role_to_user ( username ) cls . _generate_token_for_user ( username ) cls . _init_api_limit_for_user ( username ) return { \"success\" : True , \"message\" : \"User created successfully.\" , \"username\" : username , } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} @classmethod def check_if_user_exist ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Checks if a user exists in the database. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the check. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT id FROM users WHERE username = ' { username } '; \"\"\" ) match = cursor . fetchone () cls . _disconnect () return { \"success\" : True } if match else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} @classmethod def get_api_token ( cls , username : str ) -> Dict [ str , str ]: \"\"\" Gets the API token of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the API token of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT token FROM api_tokens JOIN users ON users.id = api_tokens.user_id WHERE users.username = ' { username } '; \"\"\" ) token = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"token\" : token [ 0 ]} if token else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )} @classmethod def increment_user_api_consumption ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Increments the API consumption of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the increment. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" UPDATE user_api_consumptions SET api_consumption = api_consumption + 1 WHERE user_id = (SELECT id FROM users WHERE username = ' { username } '); \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"error\" : str ( e )} @classmethod def get_user_api_consumption ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Gets the API consumption of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the API consumption of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT api_consumption FROM user_api_consumptions WHERE user_id = (SELECT id FROM users WHERE username = ' { username } '); \"\"\" ) consumption = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"consumption\" : consumption [ 0 ]} except Exception as e : return { \"error\" : str ( e )} @classmethod def _check_user_role ( cls , username : str ) -> Dict [ str , str ]: \"\"\" Checks the role of the user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the role of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT name FROM user_roles JOIN users ON users.id = user_roles.user_id JOIN roles ON roles.id = user_roles.role_id WHERE users.username = ' { username } '; \"\"\" ) role = cursor . fetchone ()[ 0 ] cls . _disconnect () return { \"role\" : role } except Exception as e : return { \"error\" : str ( e )} @classmethod def _add_member_role_to_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Adds the role of the user to the database. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO user_roles (user_id, role_id) SELECT u.id, r.id FROM (SELECT id FROM users WHERE username = ' { username } ') u, (SELECT id FROM roles WHERE name = 'member') r; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} @classmethod def _generate_token_for_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Generates a token for the user. Parameters ---------- username: str The username of the user. Returns ------- str: The token of the user. \"\"\" token = random_string () try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO api_tokens (user_id, token) SELECT u.id, ' { token } ' FROM (SELECT id FROM users WHERE username = ' { username } ') u; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} @classmethod def _init_api_limit_for_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Initializes the API limit for the user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO user_api_consumptions (user_id, api_consumption) SELECT u.id, 0 FROM (SELECT id FROM users WHERE username = ' { username } ') u; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )} @classmethod def _connect ( cls ): \"\"\" Connects to the database. \"\"\" try : cls . connection = psycopg2 . connect ( database = getenv ( \"POSTGRES_DB\" ), user = getenv ( \"POSTGRES_USER\" ), host = getenv ( \"HOST\" ), password = getenv ( \"POSTGRES_PASSWORD\" ) ) except Exception as e : return e @classmethod def _disconnect ( cls ): \"\"\" Closes the connection to the database. \"\"\" try : cls . connection . close () except Exception as e : return e","title":"DatabaseHandler"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler._add_member_role_to_user","text":"Adds the role of the user to the database. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the creation. Source code in make_us_rich/interface/database_handler.py 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 @classmethod def _add_member_role_to_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Adds the role of the user to the database. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO user_roles (user_id, role_id) SELECT u.id, r.id FROM (SELECT id FROM users WHERE username = ' { username } ') u, (SELECT id FROM roles WHERE name = 'member') r; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )}","title":"_add_member_role_to_user()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler._check_user_role","text":"Checks the role of the user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , str ] A dictionary with the role of the user. Source code in make_us_rich/interface/database_handler.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 @classmethod def _check_user_role ( cls , username : str ) -> Dict [ str , str ]: \"\"\" Checks the role of the user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the role of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT name FROM user_roles JOIN users ON users.id = user_roles.user_id JOIN roles ON roles.id = user_roles.role_id WHERE users.username = ' { username } '; \"\"\" ) role = cursor . fetchone ()[ 0 ] cls . _disconnect () return { \"role\" : role } except Exception as e : return { \"error\" : str ( e )}","title":"_check_user_role()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler._connect","text":"Connects to the database. Source code in make_us_rich/interface/database_handler.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 @classmethod def _connect ( cls ): \"\"\" Connects to the database. \"\"\" try : cls . connection = psycopg2 . connect ( database = getenv ( \"POSTGRES_DB\" ), user = getenv ( \"POSTGRES_USER\" ), host = getenv ( \"HOST\" ), password = getenv ( \"POSTGRES_PASSWORD\" ) ) except Exception as e : return e","title":"_connect()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler._disconnect","text":"Closes the connection to the database. Source code in make_us_rich/interface/database_handler.py 338 339 340 341 342 343 344 345 346 @classmethod def _disconnect ( cls ): \"\"\" Closes the connection to the database. \"\"\" try : cls . connection . close () except Exception as e : return e","title":"_disconnect()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler._generate_token_for_user","text":"Generates a token for the user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description str Dict [ str , Any ] The token of the user. Source code in make_us_rich/interface/database_handler.py 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 @classmethod def _generate_token_for_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Generates a token for the user. Parameters ---------- username: str The username of the user. Returns ------- str: The token of the user. \"\"\" token = random_string () try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO api_tokens (user_id, token) SELECT u.id, ' { token } ' FROM (SELECT id FROM users WHERE username = ' { username } ') u; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )}","title":"_generate_token_for_user()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler._init_api_limit_for_user","text":"Initializes the API limit for the user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the creation. Source code in make_us_rich/interface/database_handler.py 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 @classmethod def _init_api_limit_for_user ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Initializes the API limit for the user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO user_api_consumptions (user_id, api_consumption) SELECT u.id, 0 FROM (SELECT id FROM users WHERE username = ' { username } ') u; \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"success\" : False , \"message\" : str ( e )}","title":"_init_api_limit_for_user()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler.authentication","text":"Checks if the user and password are correct Parameters: Name Type Description Default username str The username of the user. required password str The password of the user. required Returns: Name Type Description dict Dict [ str , bool ] A dictionary with the success of the authentication. Source code in make_us_rich/interface/database_handler.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 @classmethod def authentication ( cls , username : str , password : str ) -> Dict [ str , bool ]: \"\"\" Checks if the user and password are correct Parameters ---------- username: str The username of the user. password: str The password of the user. Returns ------- dict: A dictionary with the success of the authentication. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT id FROM users WHERE username = ' { username } ' AND password = crypt(' { password } ', password); \"\"\" ) match = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"message\" : \"Authentication successful.\" , \"username\" : username } if match else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )}","title":"authentication()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler.check_if_user_exist","text":"Checks if a user exists in the database. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the check. Source code in make_us_rich/interface/database_handler.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 @classmethod def check_if_user_exist ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Checks if a user exists in the database. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the check. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT id FROM users WHERE username = ' { username } '; \"\"\" ) match = cursor . fetchone () cls . _disconnect () return { \"success\" : True } if match else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )}","title":"check_if_user_exist()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler.create_user","text":"Creates a new user in the database. Parameters: Name Type Description Default username str The username of the new user. required password str The password of the new user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the creation. Source code in make_us_rich/interface/database_handler.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @classmethod def create_user ( cls , username : str , password : str ) -> Dict [ str , Any ]: \"\"\" Creates a new user in the database. Parameters ---------- username: str The username of the new user. password: str The password of the new user. Returns ------- dict: A dictionary with the success of the creation. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" INSERT INTO users (username, password) VALUES (' { username } ', crypt(' { password } ', gen_salt('bf'))) ON CONFLICT (username) DO NOTHING; \"\"\" ) cls . connection . commit () cls . _disconnect () cls . _add_member_role_to_user ( username ) cls . _generate_token_for_user ( username ) cls . _init_api_limit_for_user ( username ) return { \"success\" : True , \"message\" : \"User created successfully.\" , \"username\" : username , } except Exception as e : return { \"success\" : False , \"message\" : str ( e )}","title":"create_user()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler.get_api_token","text":"Gets the API token of a user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , str ] A dictionary with the API token of the user. Source code in make_us_rich/interface/database_handler.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 @classmethod def get_api_token ( cls , username : str ) -> Dict [ str , str ]: \"\"\" Gets the API token of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the API token of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT token FROM api_tokens JOIN users ON users.id = api_tokens.user_id WHERE users.username = ' { username } '; \"\"\" ) token = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"token\" : token [ 0 ]} if token else { \"success\" : False } except Exception as e : return { \"error\" : str ( e )}","title":"get_api_token()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler.get_user_api_consumption","text":"Gets the API consumption of a user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the API consumption of the user. Source code in make_us_rich/interface/database_handler.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 @classmethod def get_user_api_consumption ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Gets the API consumption of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the API consumption of the user. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" SELECT api_consumption FROM user_api_consumptions WHERE user_id = (SELECT id FROM users WHERE username = ' { username } '); \"\"\" ) consumption = cursor . fetchone () cls . _disconnect () return { \"success\" : True , \"consumption\" : consumption [ 0 ]} except Exception as e : return { \"error\" : str ( e )}","title":"get_user_api_consumption()"},{"location":"api/interface/#make_us_rich.interface.database_handler.DatabaseHandler.increment_user_api_consumption","text":"Increments the API consumption of a user. Parameters: Name Type Description Default username str The username of the user. required Returns: Name Type Description dict Dict [ str , Any ] A dictionary with the success of the increment. Source code in make_us_rich/interface/database_handler.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 @classmethod def increment_user_api_consumption ( cls , username : str ) -> Dict [ str , Any ]: \"\"\" Increments the API consumption of a user. Parameters ---------- username: str The username of the user. Returns ------- dict: A dictionary with the success of the increment. \"\"\" try : cls . _connect () cursor = cls . connection . cursor () cursor . execute ( f \"\"\" UPDATE user_api_consumptions SET api_consumption = api_consumption + 1 WHERE user_id = (SELECT id FROM users WHERE username = ' { username } '); \"\"\" ) cls . connection . commit () cls . _disconnect () return { \"success\" : True } except Exception as e : return { \"error\" : str ( e )}","title":"increment_user_api_consumption()"},{"location":"api/interface/#plots","text":"","title":"Plots"},{"location":"api/interface/#make_us_rich.interface.plots.candlestick_plot","text":"Create candlestick plot. Parameters: Name Type Description Default data pd . DataFrame Dataframe containing the data to plot. required currency str Currency to plot. required compare str Currency to compare. required pred float Prediction to plot. required Returns: Type Description go . Figure Plotly figure. Source code in make_us_rich/interface/plots.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def candlestick_plot ( data : pd . DataFrame , currency : str , compare : str , pred : float ) -> go . Figure : \"\"\" Create candlestick plot. Parameters ---------- data : pd.DataFrame Dataframe containing the data to plot. currency : str Currency to plot. compare : str Currency to compare. pred : float Prediction to plot. Returns ------- go.Figure Plotly figure. \"\"\" fig = go . Figure () fig . add_trace ( go . Candlestick ( x = data [ \"timestamp\" ], open = data [ \"open\" ], high = data [ \"high\" ], low = data [ \"low\" ], close = data [ \"close\" ], name = \"Candlestick\" ) ) fig . update_layout ( title = f \" { currency . upper () } / { compare . upper () } - last 5 days\" , yaxis_title = f \" { currency . upper () } Price\" , ) pred = float ( data . iloc [ - 1 ][ \"close\" ]) + pred timestamp = data [ \"timestamp\" ] . max () + timedelta ( hours = 1 ) fig . add_trace ( go . Scatter ( x = [ timestamp ], y = [ pred ], mode = \"markers\" , marker_color = \"red\" , name = \"Prediction\" ) ) fig . update_layout ( title = f \" { currency . upper () } / { compare . upper () } - last 5 days\" , yaxis_title = f \" { currency . upper () } Price\" , annotations = [ go . Annotation ( x = timestamp , y = pred , text = f \"Prediction: { pred : .3f } \" , showarrow = False , ) ], ) return fig","title":"candlestick_plot()"},{"location":"api/interface/#make_us_rich.interface.plots.format_data","text":"Format data from API response before plotting. Parameters: Name Type Description Default data Dict [ str , Any ] Data from API response. required Returns: Type Description Tuple [ pd . DataFrame , float ] Dataframe containing the data to plot and the prediction. Source code in make_us_rich/interface/plots.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def format_data ( data : Dict [ str , Any ]) -> Tuple [ pd . DataFrame , float ]: \"\"\" Format data from API response before plotting. Parameters ---------- data : Dict[str, Any] Data from API response. Returns ------- Tuple[pd.DataFrame, float] Dataframe containing the data to plot and the prediction. \"\"\" pred = data [ \"prediction\" ] df = pd . DataFrame ( data = data [ \"data\" ], columns = [ \"timestamp\" , \"open\" , \"high\" , \"low\" , \"close\" , \"volume\" , \"close_time\" , \"quote_av\" , \"trades\" , \"tb_base_av\" , \"tb_quote_av\" , \"ignore\" , ], ) df [ \"timestamp\" ] = pd . to_datetime ( df [ \"timestamp\" ]) df [ \"close\" ] = pd . to_numeric ( df [ \"close\" ], downcast = \"float\" ) df [ \"open\" ] = pd . to_numeric ( df [ \"open\" ], downcast = \"float\" ) df [ \"high\" ] = pd . to_numeric ( df [ \"high\" ], downcast = \"float\" ) df [ \"low\" ] = pd . to_numeric ( df [ \"low\" ], downcast = \"float\" ) return df , pred","title":"format_data()"},{"location":"api/interface/#make_us_rich.interface.plots.scatter_plot","text":"Create scatter plot. Parameters: Name Type Description Default data pd . DataFrame Dataframe containing the data to plot. required currency str Currency to plot. required compare str Currency to compare. required pred float Prediction to plot. required Returns: Type Description go . Figure Plotly figure. Source code in make_us_rich/interface/plots.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def scatter_plot ( data : pd . DataFrame , currency : str , compare : str , pred : float ) -> go . Figure : \"\"\" Create scatter plot. Parameters ---------- data : pd.DataFrame Dataframe containing the data to plot. currency : str Currency to plot. compare : str Currency to compare. pred : float Prediction to plot. Returns ------- go.Figure Plotly figure. \"\"\" fig = go . Figure () fig . add_trace ( go . Scatter ( x = data [ \"timestamp\" ], y = data [ \"close\" ], name = \"Close Price\" , line_color = \"blue\" , connectgaps = True ) ) fig . update_layout ( title = f \" { currency . upper () } / { compare . upper () } - last 5 days\" , yaxis_title = f \" { currency . upper () } Price\" , ) pred = float ( data . iloc [ - 1 ][ \"close\" ]) + pred timestamp = data [ \"timestamp\" ] . max () + timedelta ( hours = 1 ) fig . add_trace ( go . Scatter ( x = [ timestamp ], y = [ pred ], mode = \"markers\" , marker_color = \"red\" , name = \"Prediction\" ) ) fig . update_layout ( title = f \" { currency . upper () } / { compare . upper () } - last 5 days\" , yaxis_title = f \" { currency . upper () } Price\" , annotations = [ go . Annotation ( x = timestamp , y = pred , text = f \"Prediction: { pred : .3f } \" , showarrow = False , ) ], ) return fig","title":"scatter_plot()"},{"location":"api/serving/","text":"ModelLoader Loader class for interacting with the Minio Object Storage API. Source code in make_us_rich/serving/model_loader.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class ModelLoader : \"\"\" Loader class for interacting with the Minio Object Storage API. \"\"\" def __init__ ( self ): self . client = MinioClient () self . session_models = {} self . storage_path = Path . cwd () . joinpath ( \"api\" , \"models\" ) self . update_date () self . update_model_files () def get_predictions ( self , model_name : str , sample : pd . DataFrame ) -> float : \"\"\" Gets the predictions from the model. Parameters ---------- model_name: str Name of the model. sample: pd.DataFrame Sample to predict. Returns ------- float Predicted value. \"\"\" if self . _check_model_exists_in_session ( model_name ): model = self . session_models [ model_name ][ \"model\" ] return model . predict ( sample ) else : raise ValueError ( \"Model not found in session.\" ) def update_date ( self ): \"\"\" Updates the date of the loader. \"\"\" self . date = datetime . now () . strftime ( \"%Y-%m- %d \" ) def update_model_files ( self ): \"\"\" Updates the model files in the serving models directory. \"\"\" for model in self . _get_list_of_available_models (): currency , compare = model . split ( \"_\" ) self . _download_files ( currency , compare ) self . _add_model_to_session_models ( currency , compare ) def _get_models_files_path ( self , currency : str , compare : str ): \"\"\" Returns the path to the files in models directory. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. Returns ------- str Path to the model files. \"\"\" model = self . storage_path . joinpath ( f \" { currency } _ { compare } \" , \"model.onnx\" ) scaler = self . storage_path . joinpath ( f \" { currency } _ { compare } \" , \"scaler.pkl\" ) return model , scaler def _makedir ( self , currency : str , compare : str ) -> None : \"\"\" Creates a directory for the model files if it doesn't exist. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. \"\"\" self . storage_path . joinpath ( f \" { currency } _ { compare } \" ) . mkdir ( exist_ok = True ) def _download_files ( self , currency : str , compare : str ) -> None : \"\"\" Downloads model and features engineering files from Minio. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. \"\"\" self . _makedir ( currency , compare ) self . client . download ( self . client . bucket , f \" { self . date } / { currency } _ { compare } /model.onnx\" , f \" { self . storage_path } / { currency } _ { compare } /model.onnx\" ) self . client . download ( self . client . bucket , f \" { self . date } / { currency } _ { compare } /scaler.pkl\" , f \" { self . storage_path } / { currency } _ { compare } /scaler.pkl\" ) def _get_list_of_available_models ( self ) -> List [ str ]: \"\"\" Looks for available models in the Minio bucket based on the date. Returns ------- List[str] List of available models. \"\"\" available_models = self . client . list_objects ( self . client . bucket , prefix = self . date , recursive = True ) return list ( set ([ model . object_name . split ( \"/\" )[ 1 ] for model in available_models ])) def _add_model_to_session_models ( self , currency : str , compare : str ) -> str : \"\"\" Adds a new model to the model session. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. Returns ------- str \"\"\" model_path , scaler_path = self . _get_models_files_path ( currency , compare ) model = OnnxModel ( model_path = model_path , scaler_path = scaler_path ) self . session_models [ f \" { currency } _ { compare } \" ] = { \"model\" : model } return f \"Model { model } added to session.\" def _check_model_exists_in_session ( self , model_name : str ) -> bool : \"\"\" Checks if the model exists in the current session. Parameters ---------- model_name: str Name of the model. Returns ------- bool \"\"\" if model_name in self . session_models . keys (): return True return False _add_model_to_session_models ( currency , compare ) Adds a new model to the model session. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required Returns: Type Description str Source code in make_us_rich/serving/model_loader.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def _add_model_to_session_models ( self , currency : str , compare : str ) -> str : \"\"\" Adds a new model to the model session. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. Returns ------- str \"\"\" model_path , scaler_path = self . _get_models_files_path ( currency , compare ) model = OnnxModel ( model_path = model_path , scaler_path = scaler_path ) self . session_models [ f \" { currency } _ { compare } \" ] = { \"model\" : model } return f \"Model { model } added to session.\" _check_model_exists_in_session ( model_name ) Checks if the model exists in the current session. Parameters: Name Type Description Default model_name str Name of the model. required Returns: Type Description bool Source code in make_us_rich/serving/model_loader.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def _check_model_exists_in_session ( self , model_name : str ) -> bool : \"\"\" Checks if the model exists in the current session. Parameters ---------- model_name: str Name of the model. Returns ------- bool \"\"\" if model_name in self . session_models . keys (): return True return False _download_files ( currency , compare ) Downloads model and features engineering files from Minio. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required Source code in make_us_rich/serving/model_loader.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def _download_files ( self , currency : str , compare : str ) -> None : \"\"\" Downloads model and features engineering files from Minio. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. \"\"\" self . _makedir ( currency , compare ) self . client . download ( self . client . bucket , f \" { self . date } / { currency } _ { compare } /model.onnx\" , f \" { self . storage_path } / { currency } _ { compare } /model.onnx\" ) self . client . download ( self . client . bucket , f \" { self . date } / { currency } _ { compare } /scaler.pkl\" , f \" { self . storage_path } / { currency } _ { compare } /scaler.pkl\" ) _get_list_of_available_models () Looks for available models in the Minio bucket based on the date. Returns: Type Description List [ str ] List of available models. Source code in make_us_rich/serving/model_loader.py 123 124 125 126 127 128 129 130 131 132 133 def _get_list_of_available_models ( self ) -> List [ str ]: \"\"\" Looks for available models in the Minio bucket based on the date. Returns ------- List[str] List of available models. \"\"\" available_models = self . client . list_objects ( self . client . bucket , prefix = self . date , recursive = True ) return list ( set ([ model . object_name . split ( \"/\" )[ 1 ] for model in available_models ])) _get_models_files_path ( currency , compare ) Returns the path to the files in models directory. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required Returns: Type Description str Path to the model files. Source code in make_us_rich/serving/model_loader.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def _get_models_files_path ( self , currency : str , compare : str ): \"\"\" Returns the path to the files in models directory. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. Returns ------- str Path to the model files. \"\"\" model = self . storage_path . joinpath ( f \" { currency } _ { compare } \" , \"model.onnx\" ) scaler = self . storage_path . joinpath ( f \" { currency } _ { compare } \" , \"scaler.pkl\" ) return model , scaler _makedir ( currency , compare ) Creates a directory for the model files if it doesn't exist. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required Source code in make_us_rich/serving/model_loader.py 85 86 87 88 89 90 91 92 93 94 95 96 def _makedir ( self , currency : str , compare : str ) -> None : \"\"\" Creates a directory for the model files if it doesn't exist. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. \"\"\" self . storage_path . joinpath ( f \" { currency } _ { compare } \" ) . mkdir ( exist_ok = True ) get_predictions ( model_name , sample ) Gets the predictions from the model. Parameters: Name Type Description Default model_name str Name of the model. required sample pd . DataFrame Sample to predict. required Returns: Type Description float Predicted value. Source code in make_us_rich/serving/model_loader.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def get_predictions ( self , model_name : str , sample : pd . DataFrame ) -> float : \"\"\" Gets the predictions from the model. Parameters ---------- model_name: str Name of the model. sample: pd.DataFrame Sample to predict. Returns ------- float Predicted value. \"\"\" if self . _check_model_exists_in_session ( model_name ): model = self . session_models [ model_name ][ \"model\" ] return model . predict ( sample ) else : raise ValueError ( \"Model not found in session.\" ) update_date () Updates the date of the loader. Source code in make_us_rich/serving/model_loader.py 47 48 49 50 51 def update_date ( self ): \"\"\" Updates the date of the loader. \"\"\" self . date = datetime . now () . strftime ( \"%Y-%m- %d \" ) update_model_files () Updates the model files in the serving models directory. Source code in make_us_rich/serving/model_loader.py 54 55 56 57 58 59 60 61 def update_model_files ( self ): \"\"\" Updates the model files in the serving models directory. \"\"\" for model in self . _get_list_of_available_models (): currency , compare = model . split ( \"_\" ) self . _download_files ( currency , compare ) self . _add_model_to_session_models ( currency , compare ) OnnxModel Source code in make_us_rich/serving/model.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class OnnxModel : def __init__ ( self , model_path : PosixPath , scaler_path : PosixPath ): self . model_path = model_path self . scaler_path = scaler_path self . model_name = self . model_path . parent . parts [ - 1 ] self . model = onnxruntime . InferenceSession ( str ( model_path )) self . scaler = self . _load_scaler () self . descaler = self . _create_descaler () def __repr__ ( self ) -> str : return f \"<OnnxModel: { self . model_name } >\" def predict ( self , sample : pd . DataFrame ) -> float : \"\"\" Predicts the close price based on the input sample. Parameters ---------- sample: pd.DataFrame Input sample. Returns ------- float Predicted close price. \"\"\" X = self . _preprocessing_sample ( sample ) inputs = { self . model . get_inputs ()[ 0 ] . name : to_numpy ( X )} results = self . model . run ( None , inputs )[ 0 ][ 0 ] return self . _descaling_sample ( results ) def _create_descaler ( self ) -> MinMaxScaler : \"\"\" Creates a descaler. Returns ------- MinMaxScaler \"\"\" descaler = MinMaxScaler () descaler . min_ , descaler . scale_ = self . scaler . min_ [ - 1 ], self . scaler . scale_ [ - 1 ] return descaler def _descaling_sample ( self , sample ) -> None : \"\"\" Descalings the sample. Parameters ---------- sample: numpy.ndarray Sample to be descaled. Returns ------- float Descaled sample. \"\"\" values_2d = np . array ( sample )[:, np . newaxis ] return self . descaler . inverse_transform ( values_2d ) . flatten () def _load_scaler ( self ) -> MinMaxScaler : \"\"\" Loads the scaler from the model files. Returns ------- MinMaxScaler \"\"\" with open ( self . scaler_path , \"rb\" ) as file : return load ( file ) def _preprocessing_sample ( self , sample : pd . DataFrame ) -> torch . tensor : \"\"\" Preprocesses the input sample. Parameters ---------- sample: pd.DataFrame Input sample. Returns ------- torch.tensor Preprocessed sample. \"\"\" data = extract_features_from_dataset ( sample ) scaled_data = pd . DataFrame ( self . scaler . transform ( data ), index = data . index , columns = data . columns ) return torch . Tensor ( scaled_data . values ) . unsqueeze ( 0 ) _create_descaler () Creates a descaler. Returns: Type Description MinMaxScaler Source code in make_us_rich/serving/model.py 49 50 51 52 53 54 55 56 57 58 59 def _create_descaler ( self ) -> MinMaxScaler : \"\"\" Creates a descaler. Returns ------- MinMaxScaler \"\"\" descaler = MinMaxScaler () descaler . min_ , descaler . scale_ = self . scaler . min_ [ - 1 ], self . scaler . scale_ [ - 1 ] return descaler _descaling_sample ( sample ) Descalings the sample. Parameters: Name Type Description Default sample Sample to be descaled. required Returns: Type Description float Descaled sample. Source code in make_us_rich/serving/model.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def _descaling_sample ( self , sample ) -> None : \"\"\" Descalings the sample. Parameters ---------- sample: numpy.ndarray Sample to be descaled. Returns ------- float Descaled sample. \"\"\" values_2d = np . array ( sample )[:, np . newaxis ] return self . descaler . inverse_transform ( values_2d ) . flatten () _load_scaler () Loads the scaler from the model files. Returns: Type Description MinMaxScaler Source code in make_us_rich/serving/model.py 80 81 82 83 84 85 86 87 88 89 def _load_scaler ( self ) -> MinMaxScaler : \"\"\" Loads the scaler from the model files. Returns ------- MinMaxScaler \"\"\" with open ( self . scaler_path , \"rb\" ) as file : return load ( file ) _preprocessing_sample ( sample ) Preprocesses the input sample. Parameters: Name Type Description Default sample pd . DataFrame Input sample. required Returns: Type Description torch . tensor Preprocessed sample. Source code in make_us_rich/serving/model.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def _preprocessing_sample ( self , sample : pd . DataFrame ) -> torch . tensor : \"\"\" Preprocesses the input sample. Parameters ---------- sample: pd.DataFrame Input sample. Returns ------- torch.tensor Preprocessed sample. \"\"\" data = extract_features_from_dataset ( sample ) scaled_data = pd . DataFrame ( self . scaler . transform ( data ), index = data . index , columns = data . columns ) return torch . Tensor ( scaled_data . values ) . unsqueeze ( 0 ) predict ( sample ) Predicts the close price based on the input sample. Parameters: Name Type Description Default sample pd . DataFrame Input sample. required Returns: Type Description float Predicted close price. Source code in make_us_rich/serving/model.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def predict ( self , sample : pd . DataFrame ) -> float : \"\"\" Predicts the close price based on the input sample. Parameters ---------- sample: pd.DataFrame Input sample. Returns ------- float Predicted close price. \"\"\" X = self . _preprocessing_sample ( sample ) inputs = { self . model . get_inputs ()[ 0 ] . name : to_numpy ( X )} results = self . model . run ( None , inputs )[ 0 ][ 0 ] return self . _descaling_sample ( results )","title":"serving"},{"location":"api/serving/#modelloader","text":"Loader class for interacting with the Minio Object Storage API. Source code in make_us_rich/serving/model_loader.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 class ModelLoader : \"\"\" Loader class for interacting with the Minio Object Storage API. \"\"\" def __init__ ( self ): self . client = MinioClient () self . session_models = {} self . storage_path = Path . cwd () . joinpath ( \"api\" , \"models\" ) self . update_date () self . update_model_files () def get_predictions ( self , model_name : str , sample : pd . DataFrame ) -> float : \"\"\" Gets the predictions from the model. Parameters ---------- model_name: str Name of the model. sample: pd.DataFrame Sample to predict. Returns ------- float Predicted value. \"\"\" if self . _check_model_exists_in_session ( model_name ): model = self . session_models [ model_name ][ \"model\" ] return model . predict ( sample ) else : raise ValueError ( \"Model not found in session.\" ) def update_date ( self ): \"\"\" Updates the date of the loader. \"\"\" self . date = datetime . now () . strftime ( \"%Y-%m- %d \" ) def update_model_files ( self ): \"\"\" Updates the model files in the serving models directory. \"\"\" for model in self . _get_list_of_available_models (): currency , compare = model . split ( \"_\" ) self . _download_files ( currency , compare ) self . _add_model_to_session_models ( currency , compare ) def _get_models_files_path ( self , currency : str , compare : str ): \"\"\" Returns the path to the files in models directory. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. Returns ------- str Path to the model files. \"\"\" model = self . storage_path . joinpath ( f \" { currency } _ { compare } \" , \"model.onnx\" ) scaler = self . storage_path . joinpath ( f \" { currency } _ { compare } \" , \"scaler.pkl\" ) return model , scaler def _makedir ( self , currency : str , compare : str ) -> None : \"\"\" Creates a directory for the model files if it doesn't exist. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. \"\"\" self . storage_path . joinpath ( f \" { currency } _ { compare } \" ) . mkdir ( exist_ok = True ) def _download_files ( self , currency : str , compare : str ) -> None : \"\"\" Downloads model and features engineering files from Minio. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. \"\"\" self . _makedir ( currency , compare ) self . client . download ( self . client . bucket , f \" { self . date } / { currency } _ { compare } /model.onnx\" , f \" { self . storage_path } / { currency } _ { compare } /model.onnx\" ) self . client . download ( self . client . bucket , f \" { self . date } / { currency } _ { compare } /scaler.pkl\" , f \" { self . storage_path } / { currency } _ { compare } /scaler.pkl\" ) def _get_list_of_available_models ( self ) -> List [ str ]: \"\"\" Looks for available models in the Minio bucket based on the date. Returns ------- List[str] List of available models. \"\"\" available_models = self . client . list_objects ( self . client . bucket , prefix = self . date , recursive = True ) return list ( set ([ model . object_name . split ( \"/\" )[ 1 ] for model in available_models ])) def _add_model_to_session_models ( self , currency : str , compare : str ) -> str : \"\"\" Adds a new model to the model session. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. Returns ------- str \"\"\" model_path , scaler_path = self . _get_models_files_path ( currency , compare ) model = OnnxModel ( model_path = model_path , scaler_path = scaler_path ) self . session_models [ f \" { currency } _ { compare } \" ] = { \"model\" : model } return f \"Model { model } added to session.\" def _check_model_exists_in_session ( self , model_name : str ) -> bool : \"\"\" Checks if the model exists in the current session. Parameters ---------- model_name: str Name of the model. Returns ------- bool \"\"\" if model_name in self . session_models . keys (): return True return False","title":"ModelLoader"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader._add_model_to_session_models","text":"Adds a new model to the model session. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required Returns: Type Description str Source code in make_us_rich/serving/model_loader.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def _add_model_to_session_models ( self , currency : str , compare : str ) -> str : \"\"\" Adds a new model to the model session. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. Returns ------- str \"\"\" model_path , scaler_path = self . _get_models_files_path ( currency , compare ) model = OnnxModel ( model_path = model_path , scaler_path = scaler_path ) self . session_models [ f \" { currency } _ { compare } \" ] = { \"model\" : model } return f \"Model { model } added to session.\"","title":"_add_model_to_session_models()"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader._check_model_exists_in_session","text":"Checks if the model exists in the current session. Parameters: Name Type Description Default model_name str Name of the model. required Returns: Type Description bool Source code in make_us_rich/serving/model_loader.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def _check_model_exists_in_session ( self , model_name : str ) -> bool : \"\"\" Checks if the model exists in the current session. Parameters ---------- model_name: str Name of the model. Returns ------- bool \"\"\" if model_name in self . session_models . keys (): return True return False","title":"_check_model_exists_in_session()"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader._download_files","text":"Downloads model and features engineering files from Minio. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required Source code in make_us_rich/serving/model_loader.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def _download_files ( self , currency : str , compare : str ) -> None : \"\"\" Downloads model and features engineering files from Minio. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. \"\"\" self . _makedir ( currency , compare ) self . client . download ( self . client . bucket , f \" { self . date } / { currency } _ { compare } /model.onnx\" , f \" { self . storage_path } / { currency } _ { compare } /model.onnx\" ) self . client . download ( self . client . bucket , f \" { self . date } / { currency } _ { compare } /scaler.pkl\" , f \" { self . storage_path } / { currency } _ { compare } /scaler.pkl\" )","title":"_download_files()"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader._get_list_of_available_models","text":"Looks for available models in the Minio bucket based on the date. Returns: Type Description List [ str ] List of available models. Source code in make_us_rich/serving/model_loader.py 123 124 125 126 127 128 129 130 131 132 133 def _get_list_of_available_models ( self ) -> List [ str ]: \"\"\" Looks for available models in the Minio bucket based on the date. Returns ------- List[str] List of available models. \"\"\" available_models = self . client . list_objects ( self . client . bucket , prefix = self . date , recursive = True ) return list ( set ([ model . object_name . split ( \"/\" )[ 1 ] for model in available_models ]))","title":"_get_list_of_available_models()"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader._get_models_files_path","text":"Returns the path to the files in models directory. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required Returns: Type Description str Path to the model files. Source code in make_us_rich/serving/model_loader.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def _get_models_files_path ( self , currency : str , compare : str ): \"\"\" Returns the path to the files in models directory. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. Returns ------- str Path to the model files. \"\"\" model = self . storage_path . joinpath ( f \" { currency } _ { compare } \" , \"model.onnx\" ) scaler = self . storage_path . joinpath ( f \" { currency } _ { compare } \" , \"scaler.pkl\" ) return model , scaler","title":"_get_models_files_path()"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader._makedir","text":"Creates a directory for the model files if it doesn't exist. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required Source code in make_us_rich/serving/model_loader.py 85 86 87 88 89 90 91 92 93 94 95 96 def _makedir ( self , currency : str , compare : str ) -> None : \"\"\" Creates a directory for the model files if it doesn't exist. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. \"\"\" self . storage_path . joinpath ( f \" { currency } _ { compare } \" ) . mkdir ( exist_ok = True )","title":"_makedir()"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader.get_predictions","text":"Gets the predictions from the model. Parameters: Name Type Description Default model_name str Name of the model. required sample pd . DataFrame Sample to predict. required Returns: Type Description float Predicted value. Source code in make_us_rich/serving/model_loader.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def get_predictions ( self , model_name : str , sample : pd . DataFrame ) -> float : \"\"\" Gets the predictions from the model. Parameters ---------- model_name: str Name of the model. sample: pd.DataFrame Sample to predict. Returns ------- float Predicted value. \"\"\" if self . _check_model_exists_in_session ( model_name ): model = self . session_models [ model_name ][ \"model\" ] return model . predict ( sample ) else : raise ValueError ( \"Model not found in session.\" )","title":"get_predictions()"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader.update_date","text":"Updates the date of the loader. Source code in make_us_rich/serving/model_loader.py 47 48 49 50 51 def update_date ( self ): \"\"\" Updates the date of the loader. \"\"\" self . date = datetime . now () . strftime ( \"%Y-%m- %d \" )","title":"update_date()"},{"location":"api/serving/#make_us_rich.serving.model_loader.ModelLoader.update_model_files","text":"Updates the model files in the serving models directory. Source code in make_us_rich/serving/model_loader.py 54 55 56 57 58 59 60 61 def update_model_files ( self ): \"\"\" Updates the model files in the serving models directory. \"\"\" for model in self . _get_list_of_available_models (): currency , compare = model . split ( \"_\" ) self . _download_files ( currency , compare ) self . _add_model_to_session_models ( currency , compare )","title":"update_model_files()"},{"location":"api/serving/#onnxmodel","text":"Source code in make_us_rich/serving/model.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 class OnnxModel : def __init__ ( self , model_path : PosixPath , scaler_path : PosixPath ): self . model_path = model_path self . scaler_path = scaler_path self . model_name = self . model_path . parent . parts [ - 1 ] self . model = onnxruntime . InferenceSession ( str ( model_path )) self . scaler = self . _load_scaler () self . descaler = self . _create_descaler () def __repr__ ( self ) -> str : return f \"<OnnxModel: { self . model_name } >\" def predict ( self , sample : pd . DataFrame ) -> float : \"\"\" Predicts the close price based on the input sample. Parameters ---------- sample: pd.DataFrame Input sample. Returns ------- float Predicted close price. \"\"\" X = self . _preprocessing_sample ( sample ) inputs = { self . model . get_inputs ()[ 0 ] . name : to_numpy ( X )} results = self . model . run ( None , inputs )[ 0 ][ 0 ] return self . _descaling_sample ( results ) def _create_descaler ( self ) -> MinMaxScaler : \"\"\" Creates a descaler. Returns ------- MinMaxScaler \"\"\" descaler = MinMaxScaler () descaler . min_ , descaler . scale_ = self . scaler . min_ [ - 1 ], self . scaler . scale_ [ - 1 ] return descaler def _descaling_sample ( self , sample ) -> None : \"\"\" Descalings the sample. Parameters ---------- sample: numpy.ndarray Sample to be descaled. Returns ------- float Descaled sample. \"\"\" values_2d = np . array ( sample )[:, np . newaxis ] return self . descaler . inverse_transform ( values_2d ) . flatten () def _load_scaler ( self ) -> MinMaxScaler : \"\"\" Loads the scaler from the model files. Returns ------- MinMaxScaler \"\"\" with open ( self . scaler_path , \"rb\" ) as file : return load ( file ) def _preprocessing_sample ( self , sample : pd . DataFrame ) -> torch . tensor : \"\"\" Preprocesses the input sample. Parameters ---------- sample: pd.DataFrame Input sample. Returns ------- torch.tensor Preprocessed sample. \"\"\" data = extract_features_from_dataset ( sample ) scaled_data = pd . DataFrame ( self . scaler . transform ( data ), index = data . index , columns = data . columns ) return torch . Tensor ( scaled_data . values ) . unsqueeze ( 0 )","title":"OnnxModel"},{"location":"api/serving/#make_us_rich.serving.model.OnnxModel._create_descaler","text":"Creates a descaler. Returns: Type Description MinMaxScaler Source code in make_us_rich/serving/model.py 49 50 51 52 53 54 55 56 57 58 59 def _create_descaler ( self ) -> MinMaxScaler : \"\"\" Creates a descaler. Returns ------- MinMaxScaler \"\"\" descaler = MinMaxScaler () descaler . min_ , descaler . scale_ = self . scaler . min_ [ - 1 ], self . scaler . scale_ [ - 1 ] return descaler","title":"_create_descaler()"},{"location":"api/serving/#make_us_rich.serving.model.OnnxModel._descaling_sample","text":"Descalings the sample. Parameters: Name Type Description Default sample Sample to be descaled. required Returns: Type Description float Descaled sample. Source code in make_us_rich/serving/model.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def _descaling_sample ( self , sample ) -> None : \"\"\" Descalings the sample. Parameters ---------- sample: numpy.ndarray Sample to be descaled. Returns ------- float Descaled sample. \"\"\" values_2d = np . array ( sample )[:, np . newaxis ] return self . descaler . inverse_transform ( values_2d ) . flatten ()","title":"_descaling_sample()"},{"location":"api/serving/#make_us_rich.serving.model.OnnxModel._load_scaler","text":"Loads the scaler from the model files. Returns: Type Description MinMaxScaler Source code in make_us_rich/serving/model.py 80 81 82 83 84 85 86 87 88 89 def _load_scaler ( self ) -> MinMaxScaler : \"\"\" Loads the scaler from the model files. Returns ------- MinMaxScaler \"\"\" with open ( self . scaler_path , \"rb\" ) as file : return load ( file )","title":"_load_scaler()"},{"location":"api/serving/#make_us_rich.serving.model.OnnxModel._preprocessing_sample","text":"Preprocesses the input sample. Parameters: Name Type Description Default sample pd . DataFrame Input sample. required Returns: Type Description torch . tensor Preprocessed sample. Source code in make_us_rich/serving/model.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def _preprocessing_sample ( self , sample : pd . DataFrame ) -> torch . tensor : \"\"\" Preprocesses the input sample. Parameters ---------- sample: pd.DataFrame Input sample. Returns ------- torch.tensor Preprocessed sample. \"\"\" data = extract_features_from_dataset ( sample ) scaled_data = pd . DataFrame ( self . scaler . transform ( data ), index = data . index , columns = data . columns ) return torch . Tensor ( scaled_data . values ) . unsqueeze ( 0 )","title":"_preprocessing_sample()"},{"location":"api/serving/#make_us_rich.serving.model.OnnxModel.predict","text":"Predicts the close price based on the input sample. Parameters: Name Type Description Default sample pd . DataFrame Input sample. required Returns: Type Description float Predicted close price. Source code in make_us_rich/serving/model.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def predict ( self , sample : pd . DataFrame ) -> float : \"\"\" Predicts the close price based on the input sample. Parameters ---------- sample: pd.DataFrame Input sample. Returns ------- float Predicted close price. \"\"\" X = self . _preprocessing_sample ( sample ) inputs = { self . model . get_inputs ()[ 0 ] . name : to_numpy ( X )} results = self . model . run ( None , inputs )[ 0 ][ 0 ] return self . _descaling_sample ( results )","title":"predict()"},{"location":"api/utils/","text":"Utils clean_dir ( path_to_clean , exception ) Removes all files and directories in the given path if they don't match the exception list. Parameters: Name Type Description Default path_to_clean Union [ str , Path ] Directory path to clean. If it is a string, it will be converted to a Path object. required exception List [ str ] List of files and directories to keep. If a file or directory is in this list, it will not be removed. required Source code in make_us_rich/utils/directory_cleaning.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def clean_dir ( path_to_clean : Union [ str , Path ], exception : List [ str ]) -> None : \"\"\" Removes all files and directories in the given path if they don't match the exception list. Parameters ---------- path_to_clean : Union[str, Path] Directory path to clean. If it is a string, it will be converted to a Path object. exception : List[str] List of files and directories to keep. If a file or directory is in this list, it will not be removed. \"\"\" if isinstance ( path_to_clean , str ): path_to_clean = Path ( path_to_clean ) items_to_remove = [ item for item in path_to_clean . iterdir () if item . name not in exception ] for item in items_to_remove : if item . is_dir (): rmtree ( item ) else : item . unlink () directory_cleaning clean_dir ( path_to_clean , exception ) Removes all files and directories in the given path if they don't match the exception list. Parameters: Name Type Description Default path_to_clean Union [ str , Path ] Directory path to clean. If it is a string, it will be converted to a Path object. required exception List [ str ] List of files and directories to keep. If a file or directory is in this list, it will not be removed. required Source code in make_us_rich/utils/directory_cleaning.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def clean_dir ( path_to_clean : Union [ str , Path ], exception : List [ str ]) -> None : \"\"\" Removes all files and directories in the given path if they don't match the exception list. Parameters ---------- path_to_clean : Union[str, Path] Directory path to clean. If it is a string, it will be converted to a Path object. exception : List[str] List of files and directories to keep. If a file or directory is in this list, it will not be removed. \"\"\" if isinstance ( path_to_clean , str ): path_to_clean = Path ( path_to_clean ) items_to_remove = [ item for item in path_to_clean . iterdir () if item . name not in exception ] for item in items_to_remove : if item . is_dir (): rmtree ( item ) else : item . unlink () load_env All utility functions for the make_us_rich project. load_env ( file_name , config_dir = None ) Loads the environment variables from the specified .env file. You don't need to specify .env- prefix for the file name. e.g. load_env(\"my_env\") will load .env-my_env file. You can also specify the directory where the .env file is located. You need to provide the full path to the directory. Parameters: Name Type Description Default file_name str Name of the .env file. required config_dir Optional [ str ] Full path to the config directory. None Source code in make_us_rich/utils/load_env.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def load_env ( file_name : str , config_dir : Optional [ str ] = None ) -> Dict : \"\"\" Loads the environment variables from the specified .env file. You don't need to specify `.env-` prefix for the file name. e.g. `load_env(\"my_env\")` will load `.env-my_env` file. You can also specify the directory where the .env file is located. You need to provide the full path to the directory. Parameters ---------- file_name: str Name of the .env file. config_dir: str Full path to the config directory. \"\"\" env_filename = f \".env- { file_name } \" if config_dir is not None : return dotenv_values ( config_dir . joinpath ( env_filename )) else : workdir = Path . cwd () if workdir . parts [ - 1 ] not in CORRECT_WORKDIR : raise ValueError ( \"Your are trying to load the environment variables from the wrong directory. \" \"Please make sure you are inside one of the following directories: \" f \" { CORRECT_WORKDIR } , \" \"or provide the full path to the config directory by adding the `config_dir` parameter.\" ) config_path = workdir . joinpath ( \"conf\" ) if config_path . joinpath ( \"local\" , env_filename ) . exists (): config_file = dotenv_values ( config_path . joinpath ( \"local\" , env_filename )) return config_file config_path = config_path . joinpath ( \"base\" ) try : config_file = dotenv_values ( config_path . joinpath ( env_filename )) return config_file except FileNotFoundError : raise FileNotFoundError ( f \"File { env_filename } not found in { config_path } .\" \"Please make sure you have the correct .env file in the `conf/base` directory or \" \"`conf/local` directory.\" ) random_string random_string ( length = 25 ) Generate a random string of the specified length. Parameters: length: int The length of the string. Default is 25. Returns: str: The random generated string. Source code in make_us_rich/utils/random_string.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def random_string ( length : int = 25 ) -> str : \"\"\" Generate a random string of the specified length. Parameters: ----------- length: int The length of the string. Default is 25. Returns: -------- str: The random generated string. \"\"\" return '' . join ( random . choice ( CHARACTERS ) for _ in range ( length ))","title":"utils"},{"location":"api/utils/#utils","text":"","title":"Utils"},{"location":"api/utils/#make_us_rich.utils.clean_dir","text":"Removes all files and directories in the given path if they don't match the exception list. Parameters: Name Type Description Default path_to_clean Union [ str , Path ] Directory path to clean. If it is a string, it will be converted to a Path object. required exception List [ str ] List of files and directories to keep. If a file or directory is in this list, it will not be removed. required Source code in make_us_rich/utils/directory_cleaning.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def clean_dir ( path_to_clean : Union [ str , Path ], exception : List [ str ]) -> None : \"\"\" Removes all files and directories in the given path if they don't match the exception list. Parameters ---------- path_to_clean : Union[str, Path] Directory path to clean. If it is a string, it will be converted to a Path object. exception : List[str] List of files and directories to keep. If a file or directory is in this list, it will not be removed. \"\"\" if isinstance ( path_to_clean , str ): path_to_clean = Path ( path_to_clean ) items_to_remove = [ item for item in path_to_clean . iterdir () if item . name not in exception ] for item in items_to_remove : if item . is_dir (): rmtree ( item ) else : item . unlink ()","title":"clean_dir()"},{"location":"api/utils/#make_us_rich.utils.directory_cleaning","text":"","title":"directory_cleaning"},{"location":"api/utils/#make_us_rich.utils.directory_cleaning.clean_dir","text":"Removes all files and directories in the given path if they don't match the exception list. Parameters: Name Type Description Default path_to_clean Union [ str , Path ] Directory path to clean. If it is a string, it will be converted to a Path object. required exception List [ str ] List of files and directories to keep. If a file or directory is in this list, it will not be removed. required Source code in make_us_rich/utils/directory_cleaning.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def clean_dir ( path_to_clean : Union [ str , Path ], exception : List [ str ]) -> None : \"\"\" Removes all files and directories in the given path if they don't match the exception list. Parameters ---------- path_to_clean : Union[str, Path] Directory path to clean. If it is a string, it will be converted to a Path object. exception : List[str] List of files and directories to keep. If a file or directory is in this list, it will not be removed. \"\"\" if isinstance ( path_to_clean , str ): path_to_clean = Path ( path_to_clean ) items_to_remove = [ item for item in path_to_clean . iterdir () if item . name not in exception ] for item in items_to_remove : if item . is_dir (): rmtree ( item ) else : item . unlink ()","title":"clean_dir()"},{"location":"api/utils/#make_us_rich.utils.load_env","text":"All utility functions for the make_us_rich project.","title":"load_env"},{"location":"api/utils/#make_us_rich.utils.load_env.load_env","text":"Loads the environment variables from the specified .env file. You don't need to specify .env- prefix for the file name. e.g. load_env(\"my_env\") will load .env-my_env file. You can also specify the directory where the .env file is located. You need to provide the full path to the directory. Parameters: Name Type Description Default file_name str Name of the .env file. required config_dir Optional [ str ] Full path to the config directory. None Source code in make_us_rich/utils/load_env.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def load_env ( file_name : str , config_dir : Optional [ str ] = None ) -> Dict : \"\"\" Loads the environment variables from the specified .env file. You don't need to specify `.env-` prefix for the file name. e.g. `load_env(\"my_env\")` will load `.env-my_env` file. You can also specify the directory where the .env file is located. You need to provide the full path to the directory. Parameters ---------- file_name: str Name of the .env file. config_dir: str Full path to the config directory. \"\"\" env_filename = f \".env- { file_name } \" if config_dir is not None : return dotenv_values ( config_dir . joinpath ( env_filename )) else : workdir = Path . cwd () if workdir . parts [ - 1 ] not in CORRECT_WORKDIR : raise ValueError ( \"Your are trying to load the environment variables from the wrong directory. \" \"Please make sure you are inside one of the following directories: \" f \" { CORRECT_WORKDIR } , \" \"or provide the full path to the config directory by adding the `config_dir` parameter.\" ) config_path = workdir . joinpath ( \"conf\" ) if config_path . joinpath ( \"local\" , env_filename ) . exists (): config_file = dotenv_values ( config_path . joinpath ( \"local\" , env_filename )) return config_file config_path = config_path . joinpath ( \"base\" ) try : config_file = dotenv_values ( config_path . joinpath ( env_filename )) return config_file except FileNotFoundError : raise FileNotFoundError ( f \"File { env_filename } not found in { config_path } .\" \"Please make sure you have the correct .env file in the `conf/base` directory or \" \"`conf/local` directory.\" )","title":"load_env()"},{"location":"api/utils/#make_us_rich.utils.random_string","text":"","title":"random_string"},{"location":"api/utils/#make_us_rich.utils.random_string.random_string","text":"Generate a random string of the specified length.","title":"random_string()"},{"location":"api/utils/#make_us_rich.utils.random_string.random_string--parameters","text":"length: int The length of the string. Default is 25.","title":"Parameters:"},{"location":"api/utils/#make_us_rich.utils.random_string.random_string--returns","text":"str: The random generated string. Source code in make_us_rich/utils/random_string.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def random_string ( length : int = 25 ) -> str : \"\"\" Generate a random string of the specified length. Parameters: ----------- length: int The length of the string. Default is 25. Returns: -------- str: The random generated string. \"\"\" return '' . join ( random . choice ( CHARACTERS ) for _ in range ( length ))","title":"Returns:"},{"location":"api/worker/","text":"KedroTask Bases: Task Kedro node as a Prefect task. Source code in make_us_rich/worker/kedro_task.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class KedroTask ( Task ): \"\"\"Kedro node as a Prefect task.\"\"\" _max_retries = 5 _retry_delay = timedelta ( minutes = 2 ) def __init__ ( self , node : Node , catalog : DataCatalog ) -> None : self . _node = node self . _catalog = catalog super () . __init__ ( name = node . name , tags = node . tags , max_retries = self . _max_retries , retry_delay = self . _retry_delay ) def run ( self ): \"\"\"Run the node.\"\"\" run_node ( self . _node , self . _catalog ) run () Run the node. Source code in make_us_rich/worker/kedro_task.py 28 29 30 def run ( self ): \"\"\"Run the node.\"\"\" run_node ( self . _node , self . _catalog ) ProjectMetadata Bases: NamedTuple Structure holding project metadata derived from pyproject.toml Source code in make_us_rich/worker/project_metadata.py 20 21 22 23 24 25 26 27 28 class ProjectMetadata ( NamedTuple ): \"\"\"Structure holding project metadata derived from `pyproject.toml`\"\"\" config_file : Path package_name : str project_name : str project_path : Path project_version : str source_dir : Path","title":"worker"},{"location":"api/worker/#kedrotask","text":"Bases: Task Kedro node as a Prefect task. Source code in make_us_rich/worker/kedro_task.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class KedroTask ( Task ): \"\"\"Kedro node as a Prefect task.\"\"\" _max_retries = 5 _retry_delay = timedelta ( minutes = 2 ) def __init__ ( self , node : Node , catalog : DataCatalog ) -> None : self . _node = node self . _catalog = catalog super () . __init__ ( name = node . name , tags = node . tags , max_retries = self . _max_retries , retry_delay = self . _retry_delay ) def run ( self ): \"\"\"Run the node.\"\"\" run_node ( self . _node , self . _catalog )","title":"KedroTask"},{"location":"api/worker/#make_us_rich.worker.kedro_task.KedroTask.run","text":"Run the node. Source code in make_us_rich/worker/kedro_task.py 28 29 30 def run ( self ): \"\"\"Run the node.\"\"\" run_node ( self . _node , self . _catalog )","title":"run()"},{"location":"api/worker/#projectmetadata","text":"Bases: NamedTuple Structure holding project metadata derived from pyproject.toml Source code in make_us_rich/worker/project_metadata.py 20 21 22 23 24 25 26 27 28 class ProjectMetadata ( NamedTuple ): \"\"\"Structure holding project metadata derived from `pyproject.toml`\"\"\" config_file : Path package_name : str project_name : str project_path : Path project_version : str source_dir : Path","title":"ProjectMetadata"},{"location":"api/pipelines/converting/","text":"Converting convert_model ( train_sequences , val_sequences , test_sequences , parameters , dir_path , training_done ) Convert trained model to ONNX. Parameters: Name Type Description Default train_sequences List [ Tuple [ pd . DataFrame , float ]] Training sequences. required val_sequences List [ Tuple [ pd . DataFrame , float ]] Validation sequences. required test_sequences List [ Tuple [ pd . DataFrame , float ]] Test sequences. required parameters str Parameters used for training. required dir_path str Directory path where the model is saved. required training_done Dict [ str , bool ] Flag indicating if the training is done. required Returns: Type Description Dict [ str , Any ] Dictionary of outputs from the conversion step. Source code in make_us_rich/pipelines/converting/nodes.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def convert_model ( train_sequences : List [ Tuple [ pd . DataFrame , float ]], val_sequences : List [ Tuple [ pd . DataFrame , float ]], test_sequences : List [ Tuple [ pd . DataFrame , float ]], parameters : str , dir_path : str , training_done : Dict [ str , bool ], ) -> Dict [ str , Any ]: \"\"\" Convert trained model to ONNX. Parameters ---------- train_sequences: List[Tuple[pd.DataFrame, float]] Training sequences. val_sequences: List[Tuple[pd.DataFrame, float]] Validation sequences. test_sequences: List[Tuple[pd.DataFrame, float]] Test sequences. parameters: str Parameters used for training. dir_path: str Directory path where the model is saved. training_done: Dict[str, bool] Flag indicating if the training is done. Returns ------- Dict[str, Any] Dictionary of outputs from the conversion step. \"\"\" if training_done [ \"training_done\" ] == True : model_path = [ file for file in glob . glob ( f \" { dir_path } /*.ckpt\" )][ 0 ] model = PricePredictor . load_from_checkpoint ( model_path ) data = LSTMDataLoader ( train_sequences = train_sequences , val_sequences = val_sequences , test_sequences = test_sequences , train_batch_size = parameters [ \"batch_size\" ], val_batch_size = parameters [ \"batch_size\" ], ) data . setup () input_batch = next ( iter ( data . train_dataloader ())) input_sample = input_batch [ 0 ][ 0 ] . unsqueeze ( 0 ) path_onnx_model = f \" { dir_path } /model.onnx\" torch . onnx . export ( model , input_sample , path_onnx_model , export_params = True , opset_version = 11 , input_names = [ \"sequence\" ], output_names = [ \"output\" ], dynamic_axes = { \"sequence\" : { 0 : \"batch_size\" }, \"output\" : { 0 : \"batch_size\" }, }, ) return { \"conversion_done\" : True , \"model_path\" : model_path , \"input_sample\" : input_sample , } to_numpy ( tensor ) Converts a tensor to numpy. Parameters: Name Type Description Default tensor torch . Tensor Tensor to be converted. required Returns: Type Description numpy . ndarray Source code in make_us_rich/pipelines/converting/nodes.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def to_numpy ( tensor : torch . Tensor ): \"\"\" Converts a tensor to numpy. Parameters ---------- tensor: torch.Tensor Tensor to be converted. Returns ------- numpy.ndarray \"\"\" return tensor . detach () . cpu () . numpy () if tensor . requires_grad else tensor . cpu () . numpy () validate_model ( dir_path , conversion_outputs ) Check if the converted model is valid. Parameters: Name Type Description Default dir_path str Directory path where the model is saved. required conversion_outputs Dict [ str , Any ] Dictionary of outputs from the conversion step. required Returns: Type Description Dict [ str , bool ] Flag indicating if the model is valid. Source code in make_us_rich/pipelines/converting/nodes.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def validate_model ( dir_path : str , conversion_outputs : Dict [ str , Any ], ) -> Dict [ str , bool ]: \"\"\" Check if the converted model is valid. Parameters ---------- dir_path: str Directory path where the model is saved. conversion_outputs: Dict[str, Any] Dictionary of outputs from the conversion step. Returns ------- Dict[str, bool] Flag indicating if the model is valid. \"\"\" if conversion_outputs [ \"conversion_done\" ] == True : path_onnx_model = f \" { dir_path } /model.onnx\" onnx_model = onnx . load ( path_onnx_model ) try : onnx . checker . check_model ( onnx_model ) except onnx . checker . ValidationError as e : raise ValueError ( f \"ONNX model is not valid: { e } \" ) try : input_sample = conversion_outputs [ \"input_sample\" ] model = PricePredictor . load_from_checkpoint ( conversion_outputs [ \"model_path\" ]) model . eval () with torch . no_grad (): torch_output = model ( input_sample ) ort_session = onnxruntime . InferenceSession ( path_onnx_model ) ort_inputs = { ort_session . get_inputs ()[ 0 ] . name : to_numpy ( input_sample )} ort_outputs = ort_session . run ( None , ort_inputs ) np . testing . assert_allclose ( to_numpy ( torch_output ), ort_outputs [ 0 ], rtol = 1e-03 , atol = 1e-05 ) print ( \"\ud83c\udf89 ONNX model is valid. \ud83c\udf89\" ) except Exception as e : raise ValueError ( f \"ONNX model is not valid: { e } \" ) return { \"validation_done\" : True }","title":"converting"},{"location":"api/pipelines/converting/#converting","text":"","title":"Converting"},{"location":"api/pipelines/converting/#make_us_rich.pipelines.converting.nodes.convert_model","text":"Convert trained model to ONNX. Parameters: Name Type Description Default train_sequences List [ Tuple [ pd . DataFrame , float ]] Training sequences. required val_sequences List [ Tuple [ pd . DataFrame , float ]] Validation sequences. required test_sequences List [ Tuple [ pd . DataFrame , float ]] Test sequences. required parameters str Parameters used for training. required dir_path str Directory path where the model is saved. required training_done Dict [ str , bool ] Flag indicating if the training is done. required Returns: Type Description Dict [ str , Any ] Dictionary of outputs from the conversion step. Source code in make_us_rich/pipelines/converting/nodes.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def convert_model ( train_sequences : List [ Tuple [ pd . DataFrame , float ]], val_sequences : List [ Tuple [ pd . DataFrame , float ]], test_sequences : List [ Tuple [ pd . DataFrame , float ]], parameters : str , dir_path : str , training_done : Dict [ str , bool ], ) -> Dict [ str , Any ]: \"\"\" Convert trained model to ONNX. Parameters ---------- train_sequences: List[Tuple[pd.DataFrame, float]] Training sequences. val_sequences: List[Tuple[pd.DataFrame, float]] Validation sequences. test_sequences: List[Tuple[pd.DataFrame, float]] Test sequences. parameters: str Parameters used for training. dir_path: str Directory path where the model is saved. training_done: Dict[str, bool] Flag indicating if the training is done. Returns ------- Dict[str, Any] Dictionary of outputs from the conversion step. \"\"\" if training_done [ \"training_done\" ] == True : model_path = [ file for file in glob . glob ( f \" { dir_path } /*.ckpt\" )][ 0 ] model = PricePredictor . load_from_checkpoint ( model_path ) data = LSTMDataLoader ( train_sequences = train_sequences , val_sequences = val_sequences , test_sequences = test_sequences , train_batch_size = parameters [ \"batch_size\" ], val_batch_size = parameters [ \"batch_size\" ], ) data . setup () input_batch = next ( iter ( data . train_dataloader ())) input_sample = input_batch [ 0 ][ 0 ] . unsqueeze ( 0 ) path_onnx_model = f \" { dir_path } /model.onnx\" torch . onnx . export ( model , input_sample , path_onnx_model , export_params = True , opset_version = 11 , input_names = [ \"sequence\" ], output_names = [ \"output\" ], dynamic_axes = { \"sequence\" : { 0 : \"batch_size\" }, \"output\" : { 0 : \"batch_size\" }, }, ) return { \"conversion_done\" : True , \"model_path\" : model_path , \"input_sample\" : input_sample , }","title":"convert_model()"},{"location":"api/pipelines/converting/#make_us_rich.pipelines.converting.nodes.to_numpy","text":"Converts a tensor to numpy. Parameters: Name Type Description Default tensor torch . Tensor Tensor to be converted. required Returns: Type Description numpy . ndarray Source code in make_us_rich/pipelines/converting/nodes.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def to_numpy ( tensor : torch . Tensor ): \"\"\" Converts a tensor to numpy. Parameters ---------- tensor: torch.Tensor Tensor to be converted. Returns ------- numpy.ndarray \"\"\" return tensor . detach () . cpu () . numpy () if tensor . requires_grad else tensor . cpu () . numpy ()","title":"to_numpy()"},{"location":"api/pipelines/converting/#make_us_rich.pipelines.converting.nodes.validate_model","text":"Check if the converted model is valid. Parameters: Name Type Description Default dir_path str Directory path where the model is saved. required conversion_outputs Dict [ str , Any ] Dictionary of outputs from the conversion step. required Returns: Type Description Dict [ str , bool ] Flag indicating if the model is valid. Source code in make_us_rich/pipelines/converting/nodes.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def validate_model ( dir_path : str , conversion_outputs : Dict [ str , Any ], ) -> Dict [ str , bool ]: \"\"\" Check if the converted model is valid. Parameters ---------- dir_path: str Directory path where the model is saved. conversion_outputs: Dict[str, Any] Dictionary of outputs from the conversion step. Returns ------- Dict[str, bool] Flag indicating if the model is valid. \"\"\" if conversion_outputs [ \"conversion_done\" ] == True : path_onnx_model = f \" { dir_path } /model.onnx\" onnx_model = onnx . load ( path_onnx_model ) try : onnx . checker . check_model ( onnx_model ) except onnx . checker . ValidationError as e : raise ValueError ( f \"ONNX model is not valid: { e } \" ) try : input_sample = conversion_outputs [ \"input_sample\" ] model = PricePredictor . load_from_checkpoint ( conversion_outputs [ \"model_path\" ]) model . eval () with torch . no_grad (): torch_output = model ( input_sample ) ort_session = onnxruntime . InferenceSession ( path_onnx_model ) ort_inputs = { ort_session . get_inputs ()[ 0 ] . name : to_numpy ( input_sample )} ort_outputs = ort_session . run ( None , ort_inputs ) np . testing . assert_allclose ( to_numpy ( torch_output ), ort_outputs [ 0 ], rtol = 1e-03 , atol = 1e-05 ) print ( \"\ud83c\udf89 ONNX model is valid. \ud83c\udf89\" ) except Exception as e : raise ValueError ( f \"ONNX model is not valid: { e } \" ) return { \"validation_done\" : True }","title":"validate_model()"},{"location":"api/pipelines/exporting/","text":"Exporting clean_files ( upload ) Cleans the files generated by the pipeline in data directory. Parameters: Name Type Description Default upload Dict [ str , bool ] Dictionary of outputs from the upload step. required Returns: Type Description Dict [ str , bool ] Dictionary of outputs from the clean step. Source code in make_us_rich/pipelines/exporting/nodes.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def clean_files ( upload : Dict [ str , bool ]) -> Dict [ str , bool ]: \"\"\" Cleans the files generated by the pipeline in data directory. Parameters ---------- upload: Dict[str, bool] Dictionary of outputs from the upload step. Returns ------- Dict[str, bool] Dictionary of outputs from the clean step. \"\"\" if upload [ \"upload_done\" ] == True : data_folders = [ folder [ 0 ] for folder in os . walk ( \"data\" ) if folder [ 0 ] != \"data\" ] for folder in data_folders : files = os . listdir ( folder ) for file in files : os . remove ( f \" { folder } / { file } \" ) if file != \".gitkeep\" else None return { \"clean_done\" : True } return { \"clean_done\" : False } upload_files ( currency , compare , dir_path , validation ) Uploads model and features engineering files to Minio. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required validation Dict [ Tuple [ str , bool ], Tuple [ str , str ]] Dictionary of outputs from the validation step. required dir_path str Directory path where the model files are saved. required Returns: Type Description Dict [ Tuple [ str , bool ]] Dictionary of outputs from the upload step. Source code in make_us_rich/pipelines/exporting/nodes.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def upload_files ( currency : str , compare : str , dir_path : str , validation : Dict [ Tuple [ str , bool ], Tuple [ str , str ]], ) -> Dict [ str , bool ]: \"\"\" Uploads model and features engineering files to Minio. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. validation: Dict[Tuple[str, bool], Tuple[str, str]] Dictionary of outputs from the validation step. dir_path: str Directory path where the model files are saved. Returns ------- Dict[Tuple[str, bool]] Dictionary of outputs from the upload step. \"\"\" if validation [ \"validation_done\" ] == True : client = MinioClient () date = datetime . now () . strftime ( \"%Y-%m- %d \" ) model_path = f \" { dir_path } /model.onnx\" client . upload ( client . bucket , f \" { date } / { currency } _ { compare } /model.onnx\" , model_path ) scaler_path = f \" { dir_path } /scaler.pkl\" client . upload ( client . bucket , f \" { date } / { currency } _ { compare } /scaler.pkl\" , scaler_path ) return { \"upload_done\" : True } return { \"upload_done\" : False }","title":"exporting"},{"location":"api/pipelines/exporting/#exporting","text":"","title":"Exporting"},{"location":"api/pipelines/exporting/#make_us_rich.pipelines.exporting.nodes.clean_files","text":"Cleans the files generated by the pipeline in data directory. Parameters: Name Type Description Default upload Dict [ str , bool ] Dictionary of outputs from the upload step. required Returns: Type Description Dict [ str , bool ] Dictionary of outputs from the clean step. Source code in make_us_rich/pipelines/exporting/nodes.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def clean_files ( upload : Dict [ str , bool ]) -> Dict [ str , bool ]: \"\"\" Cleans the files generated by the pipeline in data directory. Parameters ---------- upload: Dict[str, bool] Dictionary of outputs from the upload step. Returns ------- Dict[str, bool] Dictionary of outputs from the clean step. \"\"\" if upload [ \"upload_done\" ] == True : data_folders = [ folder [ 0 ] for folder in os . walk ( \"data\" ) if folder [ 0 ] != \"data\" ] for folder in data_folders : files = os . listdir ( folder ) for file in files : os . remove ( f \" { folder } / { file } \" ) if file != \".gitkeep\" else None return { \"clean_done\" : True } return { \"clean_done\" : False }","title":"clean_files()"},{"location":"api/pipelines/exporting/#make_us_rich.pipelines.exporting.nodes.upload_files","text":"Uploads model and features engineering files to Minio. Parameters: Name Type Description Default currency str Currency used in the model. required compare str Compare used in the model. required validation Dict [ Tuple [ str , bool ], Tuple [ str , str ]] Dictionary of outputs from the validation step. required dir_path str Directory path where the model files are saved. required Returns: Type Description Dict [ Tuple [ str , bool ]] Dictionary of outputs from the upload step. Source code in make_us_rich/pipelines/exporting/nodes.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def upload_files ( currency : str , compare : str , dir_path : str , validation : Dict [ Tuple [ str , bool ], Tuple [ str , str ]], ) -> Dict [ str , bool ]: \"\"\" Uploads model and features engineering files to Minio. Parameters ---------- currency: str Currency used in the model. compare: str Compare used in the model. validation: Dict[Tuple[str, bool], Tuple[str, str]] Dictionary of outputs from the validation step. dir_path: str Directory path where the model files are saved. Returns ------- Dict[Tuple[str, bool]] Dictionary of outputs from the upload step. \"\"\" if validation [ \"validation_done\" ] == True : client = MinioClient () date = datetime . now () . strftime ( \"%Y-%m- %d \" ) model_path = f \" { dir_path } /model.onnx\" client . upload ( client . bucket , f \" { date } / { currency } _ { compare } /model.onnx\" , model_path ) scaler_path = f \" { dir_path } /scaler.pkl\" client . upload ( client . bucket , f \" { date } / { currency } _ { compare } /scaler.pkl\" , scaler_path ) return { \"upload_done\" : True } return { \"upload_done\" : False }","title":"upload_files()"},{"location":"api/pipelines/fetching/","text":"Fetching fetch_data_to_dataframe ( currency , compare , parameters ) Fetch data from the API and convert it to a pandas dataframe. Parameters: Name Type Description Default currency str Currency to fetch data for. required compare str Currency to compare to. required parameters Dict [ str , str ] Dictionary of parameters. required Returns: Type Description pd . DataFrame Pandas dataframe containing the data. Source code in make_us_rich/pipelines/fetching/nodes.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def fetch_data_to_dataframe ( currency : str , compare : str , parameters : Dict [ str , str ], ) -> pd . DataFrame : \"\"\" Fetch data from the API and convert it to a pandas dataframe. Parameters ---------- currency: str Currency to fetch data for. compare: str Currency to compare to. parameters: Dict[str, str] Dictionary of parameters. Returns ------- pd.DataFrame Pandas dataframe containing the data. \"\"\" symbol = f \" { currency . upper () }{ compare . upper () } \" client = BinanceClient () data = client . get_data ( symbol , parameters [ \"interval\" ], parameters [ \"start_date\" ]) return data","title":"fetching"},{"location":"api/pipelines/fetching/#fetching","text":"","title":"Fetching"},{"location":"api/pipelines/fetching/#make_us_rich.pipelines.fetching.nodes.fetch_data_to_dataframe","text":"Fetch data from the API and convert it to a pandas dataframe. Parameters: Name Type Description Default currency str Currency to fetch data for. required compare str Currency to compare to. required parameters Dict [ str , str ] Dictionary of parameters. required Returns: Type Description pd . DataFrame Pandas dataframe containing the data. Source code in make_us_rich/pipelines/fetching/nodes.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def fetch_data_to_dataframe ( currency : str , compare : str , parameters : Dict [ str , str ], ) -> pd . DataFrame : \"\"\" Fetch data from the API and convert it to a pandas dataframe. Parameters ---------- currency: str Currency to fetch data for. compare: str Currency to compare to. parameters: Dict[str, str] Dictionary of parameters. Returns ------- pd.DataFrame Pandas dataframe containing the data. \"\"\" symbol = f \" { currency . upper () }{ compare . upper () } \" client = BinanceClient () data = client . get_data ( symbol , parameters [ \"interval\" ], parameters [ \"start_date\" ]) return data","title":"fetch_data_to_dataframe()"},{"location":"api/pipelines/preprocessing/","text":"Preprocessing create_sequences ( input_data , target_column , sequence_length ) Create sequences from the input data. Parameters: Name Type Description Default input_data pd . DataFrame Pandas dataframe of input data. required target_column str Name of the column to predict. required sequence_length int Length of the sequence. required Returns: Type Description List [ Tuple [ pd . DataFrame , float ]] List of sequences. Source code in make_us_rich/pipelines/preprocessing/nodes.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def create_sequences ( input_data : pd . DataFrame , target_column : str , sequence_length : int , ) -> List [ Tuple [ pd . DataFrame , float ]]: \"\"\" Create sequences from the input data. Parameters ---------- input_data: pd.DataFrame Pandas dataframe of input data. target_column: str Name of the column to predict. sequence_length: int Length of the sequence. Returns ------- List[Tuple[pd.DataFrame, float]] List of sequences. \"\"\" sequences = [] size = len ( input_data ) for i in range ( size - sequence_length ): sequence = input_data [ i : i + sequence_length ] label_position = i + sequence_length label = input_data . iloc [ label_position ][ target_column ] sequences . append ([ sequence , label ]) return sequences extract_features_from_dataset ( data ) Extract features from dataset. Parameters: Name Type Description Default data pd . DataFrame Market chart data. required Returns: Type Description pd . DataFrame Pandas dataframe of features. Source code in make_us_rich/pipelines/preprocessing/nodes.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def extract_features_from_dataset ( data : pd . DataFrame ) -> pd . DataFrame : \"\"\" Extract features from dataset. Parameters ---------- data: pd.DataFrame Market chart data. Returns ------- pd.DataFrame Pandas dataframe of features. \"\"\" rows = [] for _ , row in data . iterrows (): row_data = dict ( day_of_week = row [ \"timestamp\" ] . dayofweek , day_of_month = row [ \"timestamp\" ] . day , week_of_year = row [ \"timestamp\" ] . week , month_of_year = row [ \"timestamp\" ] . month , open = row [ \"open\" ], high = row [ \"high\" ], low = row [ \"low\" ], close = row [ \"close\" ], close_change = row [ \"close\" ] - row [ \"open\" ], ) rows . append ( row_data ) return pd . DataFrame ( rows ) scale_data ( train_df , test_df , dir_path ) Scale data to have a mean of 0 and a standard deviation of 1. Parameters: Name Type Description Default train_df pd . DataFrame Training data. required test_df pd . DataFrame Test data. required dir_path str Directory path to save the scaler. required Returns: Type Description pd . DataFrame Scaled training and test data. Source code in make_us_rich/pipelines/preprocessing/nodes.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def scale_data ( train_df : pd . DataFrame , test_df : pd . DataFrame , dir_path : str , ) -> pd . DataFrame : \"\"\" Scale data to have a mean of 0 and a standard deviation of 1. Parameters ---------- train_df: pd.DataFrame Training data. test_df: pd.DataFrame Test data. dir_path: str Directory path to save the scaler. Returns ------- pd.DataFrame Scaled training and test data. \"\"\" scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) scaler = scaler . fit ( train_df ) scaled_train_df = pd . DataFrame ( scaler . transform ( train_df ), index = train_df . index , columns = train_df . columns , ) scaled_test_df = pd . DataFrame ( scaler . transform ( test_df ), index = test_df . index , columns = test_df . columns , ) dump ( scaler , open ( f \" { dir_path } /scaler.pkl\" , \"wb\" )) return scaled_train_df , scaled_test_df split_data ( data ) Split data into training and test sets. Parameters: Name Type Description Default data pd . DataFrame Market chart data. required Returns: Type Description pd . DataFrame Pandas dataframe of training and test data. Source code in make_us_rich/pipelines/preprocessing/nodes.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def split_data ( data : pd . DataFrame ) -> pd . DataFrame : \"\"\" Split data into training and test sets. Parameters ---------- data: pd.DataFrame Market chart data. Returns ------- pd.DataFrame Pandas dataframe of training and test data. \"\"\" train_size = int ( len ( data ) * 0.9 ) train_df , test_df = data [: train_size ], data [ train_size + 1 :] return train_df , test_df split_train_and_val_sequences ( sequences , val_size ) Split sequences into training and validation sets. Parameters: Name Type Description Default sequences List [ Tuple [ pd . DataFrame , float ]] List of sequences. required val_size float Percentage of the data to use as validation. required Returns: Type Description Tuple [ List [ Tuple [ pd . DataFrame , float ]]] Tuple of training and validation sequences. Source code in make_us_rich/pipelines/preprocessing/nodes.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def split_train_and_val_sequences ( sequences : List [ Tuple [ pd . DataFrame , float ]], val_size : float , ) -> Tuple [ List [ Tuple [ pd . DataFrame , float ]]]: \"\"\" Split sequences into training and validation sets. Parameters ---------- sequences: List[Tuple[pd.DataFrame, float]] List of sequences. val_size: float Percentage of the data to use as validation. Returns ------- Tuple[List[Tuple[pd.DataFrame, float]]] Tuple of training and validation sequences. \"\"\" train_sequences , val_sequences = [], [] for sequence , label in sequences : if len ( train_sequences ) < len ( sequences ) * ( 1 - val_size ): train_sequences . append (( sequence , label )) else : val_sequences . append (( sequence , label )) return train_sequences , val_sequences","title":"preprocessing"},{"location":"api/pipelines/preprocessing/#preprocessing","text":"","title":"Preprocessing"},{"location":"api/pipelines/preprocessing/#make_us_rich.pipelines.preprocessing.nodes.create_sequences","text":"Create sequences from the input data. Parameters: Name Type Description Default input_data pd . DataFrame Pandas dataframe of input data. required target_column str Name of the column to predict. required sequence_length int Length of the sequence. required Returns: Type Description List [ Tuple [ pd . DataFrame , float ]] List of sequences. Source code in make_us_rich/pipelines/preprocessing/nodes.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def create_sequences ( input_data : pd . DataFrame , target_column : str , sequence_length : int , ) -> List [ Tuple [ pd . DataFrame , float ]]: \"\"\" Create sequences from the input data. Parameters ---------- input_data: pd.DataFrame Pandas dataframe of input data. target_column: str Name of the column to predict. sequence_length: int Length of the sequence. Returns ------- List[Tuple[pd.DataFrame, float]] List of sequences. \"\"\" sequences = [] size = len ( input_data ) for i in range ( size - sequence_length ): sequence = input_data [ i : i + sequence_length ] label_position = i + sequence_length label = input_data . iloc [ label_position ][ target_column ] sequences . append ([ sequence , label ]) return sequences","title":"create_sequences()"},{"location":"api/pipelines/preprocessing/#make_us_rich.pipelines.preprocessing.nodes.extract_features_from_dataset","text":"Extract features from dataset. Parameters: Name Type Description Default data pd . DataFrame Market chart data. required Returns: Type Description pd . DataFrame Pandas dataframe of features. Source code in make_us_rich/pipelines/preprocessing/nodes.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 def extract_features_from_dataset ( data : pd . DataFrame ) -> pd . DataFrame : \"\"\" Extract features from dataset. Parameters ---------- data: pd.DataFrame Market chart data. Returns ------- pd.DataFrame Pandas dataframe of features. \"\"\" rows = [] for _ , row in data . iterrows (): row_data = dict ( day_of_week = row [ \"timestamp\" ] . dayofweek , day_of_month = row [ \"timestamp\" ] . day , week_of_year = row [ \"timestamp\" ] . week , month_of_year = row [ \"timestamp\" ] . month , open = row [ \"open\" ], high = row [ \"high\" ], low = row [ \"low\" ], close = row [ \"close\" ], close_change = row [ \"close\" ] - row [ \"open\" ], ) rows . append ( row_data ) return pd . DataFrame ( rows )","title":"extract_features_from_dataset()"},{"location":"api/pipelines/preprocessing/#make_us_rich.pipelines.preprocessing.nodes.scale_data","text":"Scale data to have a mean of 0 and a standard deviation of 1. Parameters: Name Type Description Default train_df pd . DataFrame Training data. required test_df pd . DataFrame Test data. required dir_path str Directory path to save the scaler. required Returns: Type Description pd . DataFrame Scaled training and test data. Source code in make_us_rich/pipelines/preprocessing/nodes.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def scale_data ( train_df : pd . DataFrame , test_df : pd . DataFrame , dir_path : str , ) -> pd . DataFrame : \"\"\" Scale data to have a mean of 0 and a standard deviation of 1. Parameters ---------- train_df: pd.DataFrame Training data. test_df: pd.DataFrame Test data. dir_path: str Directory path to save the scaler. Returns ------- pd.DataFrame Scaled training and test data. \"\"\" scaler = MinMaxScaler ( feature_range = ( - 1 , 1 )) scaler = scaler . fit ( train_df ) scaled_train_df = pd . DataFrame ( scaler . transform ( train_df ), index = train_df . index , columns = train_df . columns , ) scaled_test_df = pd . DataFrame ( scaler . transform ( test_df ), index = test_df . index , columns = test_df . columns , ) dump ( scaler , open ( f \" { dir_path } /scaler.pkl\" , \"wb\" )) return scaled_train_df , scaled_test_df","title":"scale_data()"},{"location":"api/pipelines/preprocessing/#make_us_rich.pipelines.preprocessing.nodes.split_data","text":"Split data into training and test sets. Parameters: Name Type Description Default data pd . DataFrame Market chart data. required Returns: Type Description pd . DataFrame Pandas dataframe of training and test data. Source code in make_us_rich/pipelines/preprocessing/nodes.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 def split_data ( data : pd . DataFrame ) -> pd . DataFrame : \"\"\" Split data into training and test sets. Parameters ---------- data: pd.DataFrame Market chart data. Returns ------- pd.DataFrame Pandas dataframe of training and test data. \"\"\" train_size = int ( len ( data ) * 0.9 ) train_df , test_df = data [: train_size ], data [ train_size + 1 :] return train_df , test_df","title":"split_data()"},{"location":"api/pipelines/preprocessing/#make_us_rich.pipelines.preprocessing.nodes.split_train_and_val_sequences","text":"Split sequences into training and validation sets. Parameters: Name Type Description Default sequences List [ Tuple [ pd . DataFrame , float ]] List of sequences. required val_size float Percentage of the data to use as validation. required Returns: Type Description Tuple [ List [ Tuple [ pd . DataFrame , float ]]] Tuple of training and validation sequences. Source code in make_us_rich/pipelines/preprocessing/nodes.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def split_train_and_val_sequences ( sequences : List [ Tuple [ pd . DataFrame , float ]], val_size : float , ) -> Tuple [ List [ Tuple [ pd . DataFrame , float ]]]: \"\"\" Split sequences into training and validation sets. Parameters ---------- sequences: List[Tuple[pd.DataFrame, float]] List of sequences. val_size: float Percentage of the data to use as validation. Returns ------- Tuple[List[Tuple[pd.DataFrame, float]]] Tuple of training and validation sequences. \"\"\" train_sequences , val_sequences = [], [] for sequence , label in sequences : if len ( train_sequences ) < len ( sequences ) * ( 1 - val_size ): train_sequences . append (( sequence , label )) else : val_sequences . append (( sequence , label )) return train_sequences , val_sequences","title":"split_train_and_val_sequences()"},{"location":"api/pipelines/training/cryptodataset/","text":"Bases: Dataset Dataset class for the LSTM model used by PyTorch Lightning. Source code in make_us_rich/pipelines/training/crypto_dataset.py 12 13 def __init__ ( self , sequences : List [ Tuple [ pd . DataFrame , float ]]): self . sequences = sequences","title":"Cryptodataset"},{"location":"api/pipelines/training/lstmdataloader/","text":"Bases: pl . LightningDataModule Data loader for the LSTM model. Initialize the data loader. Parameters: Name Type Description Default train_sequences List [ Tuple [ pd . DataFrame , float ]] List of training sequences. required val_sequences List [ Tuple [ pd . DataFrame , float ]] List of validation sequences. required test_sequences List [ Tuple [ pd . DataFrame , float ]] List of test sequences. required train_batch_size int Batch size for training. required val_batch_size int Batch size for validation. required train_workers int Number of workers for training. 2 val_workers int Number of workers for validation. 1 Source code in make_us_rich/pipelines/training/dataloader.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __init__ ( self , train_sequences : List [ Tuple [ pd . DataFrame , float ]], val_sequences : List [ Tuple [ pd . DataFrame , float ]], test_sequences : List [ Tuple [ pd . DataFrame , float ]], train_batch_size : int , val_batch_size : int , train_workers : int = 2 , val_workers : int = 1 , ): \"\"\" Initialize the data loader. Parameters ---------- train_sequences: List[Tuple[pd.DataFrame, float]] List of training sequences. val_sequences: List[Tuple[pd.DataFrame, float]] List of validation sequences. test_sequences: List[Tuple[pd.DataFrame, float]] List of test sequences. train_batch_size: int Batch size for training. val_batch_size: int Batch size for validation. train_workers: int Number of workers for training. val_workers: int Number of workers for validation. \"\"\" super () . __init__ () self . train_sequences = train_sequences self . val_sequences = val_sequences self . test_sequences = test_sequences self . train_batch_size = train_batch_size self . val_batch_size = val_batch_size self . train_workers = train_workers self . val_workers = val_workers self . test_workers = val_workers setup ( stage = None ) Load the data. Parameters: Name Type Description Default stage str Name of the stage. None Source code in make_us_rich/pipelines/training/dataloader.py 54 55 56 57 58 59 60 61 62 63 64 65 def setup ( self , stage : str = None ) -> None : \"\"\" Load the data. Parameters ---------- stage: str Name of the stage. \"\"\" self . train_dataset = CryptoDataset ( self . train_sequences ) self . val_dataset = CryptoDataset ( self . val_sequences ) self . test_dataset = CryptoDataset ( self . test_sequences ) test_dataloader () Return the test data loader. Source code in make_us_rich/pipelines/training/dataloader.py 88 89 90 91 92 93 94 95 def test_dataloader ( self ): \"\"\"Return the test data loader.\"\"\" return DataLoader ( self . test_dataset , batch_size = self . val_batch_size , shuffle = False , num_workers = self . test_workers ) train_dataloader () Return the training data loader. Source code in make_us_rich/pipelines/training/dataloader.py 68 69 70 71 72 73 74 75 def train_dataloader ( self ): \"\"\"Return the training data loader.\"\"\" return DataLoader ( self . train_dataset , batch_size = self . train_batch_size , shuffle = False , num_workers = self . train_workers ) val_dataloader () Return the validation data loader. Source code in make_us_rich/pipelines/training/dataloader.py 78 79 80 81 82 83 84 85 def val_dataloader ( self ): \"\"\"Return the validation data loader.\"\"\" return DataLoader ( self . val_dataset , batch_size = self . val_batch_size , shuffle = False , num_workers = self . val_workers )","title":"Lstmdataloader"},{"location":"api/pipelines/training/lstmdataloader/#make_us_rich.pipelines.training.dataloader.LSTMDataLoader.setup","text":"Load the data. Parameters: Name Type Description Default stage str Name of the stage. None Source code in make_us_rich/pipelines/training/dataloader.py 54 55 56 57 58 59 60 61 62 63 64 65 def setup ( self , stage : str = None ) -> None : \"\"\" Load the data. Parameters ---------- stage: str Name of the stage. \"\"\" self . train_dataset = CryptoDataset ( self . train_sequences ) self . val_dataset = CryptoDataset ( self . val_sequences ) self . test_dataset = CryptoDataset ( self . test_sequences )","title":"setup()"},{"location":"api/pipelines/training/lstmdataloader/#make_us_rich.pipelines.training.dataloader.LSTMDataLoader.test_dataloader","text":"Return the test data loader. Source code in make_us_rich/pipelines/training/dataloader.py 88 89 90 91 92 93 94 95 def test_dataloader ( self ): \"\"\"Return the test data loader.\"\"\" return DataLoader ( self . test_dataset , batch_size = self . val_batch_size , shuffle = False , num_workers = self . test_workers )","title":"test_dataloader()"},{"location":"api/pipelines/training/lstmdataloader/#make_us_rich.pipelines.training.dataloader.LSTMDataLoader.train_dataloader","text":"Return the training data loader. Source code in make_us_rich/pipelines/training/dataloader.py 68 69 70 71 72 73 74 75 def train_dataloader ( self ): \"\"\"Return the training data loader.\"\"\" return DataLoader ( self . train_dataset , batch_size = self . train_batch_size , shuffle = False , num_workers = self . train_workers )","title":"train_dataloader()"},{"location":"api/pipelines/training/lstmdataloader/#make_us_rich.pipelines.training.dataloader.LSTMDataLoader.val_dataloader","text":"Return the validation data loader. Source code in make_us_rich/pipelines/training/dataloader.py 78 79 80 81 82 83 84 85 def val_dataloader ( self ): \"\"\"Return the validation data loader.\"\"\" return DataLoader ( self . val_dataset , batch_size = self . val_batch_size , shuffle = False , num_workers = self . val_workers )","title":"val_dataloader()"},{"location":"api/pipelines/training/lstmregressor/","text":"Bases: nn . Module Standard LSTM model with PyTorch Lightning. Source code in make_us_rich/pipelines/training/model.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , batch_size : int , dropout_rate : float , hidden_size : int , number_of_features : int , number_of_layers : int , run_on_gpu : bool , ): super () . __init__ () self . batch_size = batch_size self . dropout_rate = dropout_rate self . hidden_size = hidden_size self . n_features = number_of_features self . number_of_layers = number_of_layers self . run_on_gpu = run_on_gpu self . lstm = nn . LSTM ( batch_first = True , dropout = self . dropout_rate , hidden_size = self . hidden_size , input_size = self . n_features , num_layers = self . number_of_layers , ) self . regressor = nn . Linear ( self . hidden_size , 1 ) forward ( x ) Forward pass through the model. lstm_out = (batch_size, sequence_length, hidden_size) Source code in make_us_rich/pipelines/training/model.py 39 40 41 42 43 44 45 46 47 48 49 def forward ( self , x ): \"\"\" Forward pass through the model. lstm_out = (batch_size, sequence_length, hidden_size) \"\"\" if self . run_on_gpu : self . lstm . flatten_parameters () _ , ( hidden , _ ) = self . lstm ( x ) out = hidden [ - 1 ] return self . regressor ( out )","title":"Lstmregressor"},{"location":"api/pipelines/training/lstmregressor/#make_us_rich.pipelines.training.model.LSTMRegressor.forward","text":"Forward pass through the model. lstm_out = (batch_size, sequence_length, hidden_size) Source code in make_us_rich/pipelines/training/model.py 39 40 41 42 43 44 45 46 47 48 49 def forward ( self , x ): \"\"\" Forward pass through the model. lstm_out = (batch_size, sequence_length, hidden_size) \"\"\" if self . run_on_gpu : self . lstm . flatten_parameters () _ , ( hidden , _ ) = self . lstm ( x ) out = hidden [ - 1 ] return self . regressor ( out )","title":"forward()"},{"location":"api/pipelines/training/pricepredictor/","text":"Bases: pl . LightningModule Training model with PyTorch Lightning. Initialize the model. Parameters: Name Type Description Default batch_size int Batch size for training. required dropout_rate float Dropout rate for the LSTM. required hidden_size int Hidden size for the LSTM. required learning_rate float Learning rate for the optimizer. required number_of_features int Number of features in the input. required number_of_layers int Number of layers in the LSTM. required run_on_gpu bool Whether to run the model on the GPU. required criterion nn . Module Loss function to use. nn.MSELoss() Returns: Type Description None Source code in make_us_rich/pipelines/training/model.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , batch_size : int , dropout_rate : float , hidden_size : int , learning_rate : float , number_of_features : int , number_of_layers : int , run_on_gpu : bool , criterion : nn . Module = nn . MSELoss (), ) -> None : \"\"\" Initialize the model. Parameters ---------- batch_size: int Batch size for training. dropout_rate: float Dropout rate for the LSTM. hidden_size: int Hidden size for the LSTM. learning_rate: float Learning rate for the optimizer. number_of_features: int Number of features in the input. number_of_layers: int Number of layers in the LSTM. run_on_gpu: bool Whether to run the model on the GPU. criterion: nn.Module Loss function to use. Returns ------- None \"\"\" super () . __init__ () self . model = LSTMRegressor ( batch_size , dropout_rate , hidden_size , number_of_features , number_of_layers , run_on_gpu , ) self . learning_rate = learning_rate self . criterion = criterion self . save_hyperparameters () configure_optimizers () Configure the optimizer. Returns: Type Description torch . optim . adamw . AdamW Optimizer. Source code in make_us_rich/pipelines/training/model.py 192 193 194 195 196 197 198 199 200 201 def configure_optimizers ( self ) -> torch . optim . AdamW : \"\"\" Configure the optimizer. Returns ------- torch.optim.adamw.AdamW Optimizer. \"\"\" return torch . optim . AdamW ( self . parameters (), lr = self . learning_rate ) forward ( x , labels = None ) Forward pass through the model. Parameters: Name Type Description Default x Input data. required labels Labels for the data. None Returns: Name Type Description loss float Loss for the model. output torch . Tensor Output of the model. Source code in make_us_rich/pipelines/training/model.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def forward ( self , x , labels = None ) -> Tuple [ float , torch . Tensor ]: \"\"\" Forward pass through the model. Parameters ---------- x: torch.Tensor Input data. labels: torch.Tensor Labels for the data. Returns ------- loss: float Loss for the model. output: torch.Tensor Output of the model. \"\"\" output = self . model ( x ) if labels is not None : loss = self . criterion ( output , labels . unsqueeze ( dim = 1 )) return loss , output return output test_step ( batch , batch_idx ) Test step. Parameters: Name Type Description Default batch Tuple [ torch . Tensor , torch . Tensor ] Tuple of input data and labels. required batch_idx int Batch index. required Returns: Type Description Dict Dictionary with the test loss. Source code in make_us_rich/pipelines/training/model.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def test_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor ], batch_idx : int ) -> Dict : \"\"\" Test step. Parameters ---------- batch: Tuple[torch.Tensor, torch.Tensor] Tuple of input data and labels. batch_idx: int Batch index. Returns ------- Dict Dictionary with the test loss. \"\"\" sequences , labels = batch loss , _ = self ( sequences , labels ) self . log ( \"test/loss\" , loss , on_step = True , on_epoch = True ) return { \"loss\" : loss } training_step ( batch , batch_idx ) Training step. Parameters: Name Type Description Default batch Tuple [ torch . Tensor , torch . Tensor ] Tuple of input data and labels. required batch_idx int Batch index. required Returns: Type Description Dict Dictionary with the train loss. Source code in make_us_rich/pipelines/training/model.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor ], batch_idx : int ) -> Dict : \"\"\" Training step. Parameters ---------- batch: Tuple[torch.Tensor, torch.Tensor] Tuple of input data and labels. batch_idx: int Batch index. Returns ------- Dict Dictionary with the train loss. \"\"\" sequences , labels = batch loss , _ = self ( sequences , labels ) self . log ( \"train/loss\" , loss , on_step = True , on_epoch = True ) return { \"loss\" : loss } validation_step ( batch , batch_idx ) Validation step. Parameters: Name Type Description Default batch Tuple [ torch . Tensor , torch . Tensor ] Tuple of input data and labels. required batch_idx int Batch index. required Returns: Type Description Dict Dictionary with the valid loss. Source code in make_us_rich/pipelines/training/model.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor ], batch_idx : int ) -> Dict : \"\"\" Validation step. Parameters ---------- batch: Tuple[torch.Tensor, torch.Tensor] Tuple of input data and labels. batch_idx: int Batch index. Returns ------- Dict Dictionary with the valid loss. \"\"\" sequences , labels = batch loss , _ = self ( sequences , labels ) self . log ( \"valid/loss\" , loss , on_step = True , on_epoch = True ) return { \"loss\" : loss }","title":"Pricepredictor"},{"location":"api/pipelines/training/pricepredictor/#make_us_rich.pipelines.training.model.PricePredictor.configure_optimizers","text":"Configure the optimizer. Returns: Type Description torch . optim . adamw . AdamW Optimizer. Source code in make_us_rich/pipelines/training/model.py 192 193 194 195 196 197 198 199 200 201 def configure_optimizers ( self ) -> torch . optim . AdamW : \"\"\" Configure the optimizer. Returns ------- torch.optim.adamw.AdamW Optimizer. \"\"\" return torch . optim . AdamW ( self . parameters (), lr = self . learning_rate )","title":"configure_optimizers()"},{"location":"api/pipelines/training/pricepredictor/#make_us_rich.pipelines.training.model.PricePredictor.forward","text":"Forward pass through the model. Parameters: Name Type Description Default x Input data. required labels Labels for the data. None Returns: Name Type Description loss float Loss for the model. output torch . Tensor Output of the model. Source code in make_us_rich/pipelines/training/model.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def forward ( self , x , labels = None ) -> Tuple [ float , torch . Tensor ]: \"\"\" Forward pass through the model. Parameters ---------- x: torch.Tensor Input data. labels: torch.Tensor Labels for the data. Returns ------- loss: float Loss for the model. output: torch.Tensor Output of the model. \"\"\" output = self . model ( x ) if labels is not None : loss = self . criterion ( output , labels . unsqueeze ( dim = 1 )) return loss , output return output","title":"forward()"},{"location":"api/pipelines/training/pricepredictor/#make_us_rich.pipelines.training.model.PricePredictor.test_step","text":"Test step. Parameters: Name Type Description Default batch Tuple [ torch . Tensor , torch . Tensor ] Tuple of input data and labels. required batch_idx int Batch index. required Returns: Type Description Dict Dictionary with the test loss. Source code in make_us_rich/pipelines/training/model.py 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def test_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor ], batch_idx : int ) -> Dict : \"\"\" Test step. Parameters ---------- batch: Tuple[torch.Tensor, torch.Tensor] Tuple of input data and labels. batch_idx: int Batch index. Returns ------- Dict Dictionary with the test loss. \"\"\" sequences , labels = batch loss , _ = self ( sequences , labels ) self . log ( \"test/loss\" , loss , on_step = True , on_epoch = True ) return { \"loss\" : loss }","title":"test_step()"},{"location":"api/pipelines/training/pricepredictor/#make_us_rich.pipelines.training.model.PricePredictor.training_step","text":"Training step. Parameters: Name Type Description Default batch Tuple [ torch . Tensor , torch . Tensor ] Tuple of input data and labels. required batch_idx int Batch index. required Returns: Type Description Dict Dictionary with the train loss. Source code in make_us_rich/pipelines/training/model.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor ], batch_idx : int ) -> Dict : \"\"\" Training step. Parameters ---------- batch: Tuple[torch.Tensor, torch.Tensor] Tuple of input data and labels. batch_idx: int Batch index. Returns ------- Dict Dictionary with the train loss. \"\"\" sequences , labels = batch loss , _ = self ( sequences , labels ) self . log ( \"train/loss\" , loss , on_step = True , on_epoch = True ) return { \"loss\" : loss }","title":"training_step()"},{"location":"api/pipelines/training/pricepredictor/#make_us_rich.pipelines.training.model.PricePredictor.validation_step","text":"Validation step. Parameters: Name Type Description Default batch Tuple [ torch . Tensor , torch . Tensor ] Tuple of input data and labels. required batch_idx int Batch index. required Returns: Type Description Dict Dictionary with the valid loss. Source code in make_us_rich/pipelines/training/model.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor ], batch_idx : int ) -> Dict : \"\"\" Validation step. Parameters ---------- batch: Tuple[torch.Tensor, torch.Tensor] Tuple of input data and labels. batch_idx: int Batch index. Returns ------- Dict Dictionary with the valid loss. \"\"\" sequences , labels = batch loss , _ = self ( sequences , labels ) self . log ( \"valid/loss\" , loss , on_step = True , on_epoch = True ) return { \"loss\" : loss }","title":"validation_step()"},{"location":"api/pipelines/training/trainingloop/","text":"Training loop for the LSTM model. Parameters: Name Type Description Default train_sequences List [ Tuple [ pd . DataFrame , float ]] List of training sequences. required val_sequences List [ Tuple [ pd . DataFrame , float ]] List of validation sequences. required test_sequences List [ Tuple [ pd . DataFrame , float ]] List of test sequences. required parameters Dict [ str , Any ] Hyperparameters for the model. required dir_path str Path to the directory where the model will be saved. required Source code in make_us_rich/pipelines/training/nodes.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def training_loop ( train_sequences : List [ Tuple [ pd . DataFrame , float ]], val_sequences : List [ Tuple [ pd . DataFrame , float ]], test_sequences : List [ Tuple [ pd . DataFrame , float ]], parameters : Dict [ str , Any ], dir_path : str , ): \"\"\" Training loop for the LSTM model. Parameters ---------- train_sequences: List[Tuple[pd.DataFrame, float]] List of training sequences. val_sequences: List[Tuple[pd.DataFrame, float]] List of validation sequences. test_sequences: List[Tuple[pd.DataFrame, float]] List of test sequences. parameters: Dict[str, Any] Hyperparameters for the model. dir_path: str Path to the directory where the model will be saved. \"\"\" seed_everything ( 42 , workers = True ) logger = WandbLogger ( project = parameters [ \"wandb_project\" ]) gpu_value = 1 if parameters [ \"run_on_gpu\" ] is True else 0 model = PricePredictor ( batch_size = parameters [ \"train_batch_size\" ], dropout_rate = parameters [ \"dropout_rate\" ], hidden_size = parameters [ \"hidden_size\" ], learning_rate = parameters [ \"learning_rate\" ], number_of_features = parameters [ \"number_of_features\" ], number_of_layers = parameters [ \"number_of_layers\" ], run_on_gpu = parameters [ \"run_on_gpu\" ], ) data_module = LSTMDataLoader ( train_sequences = train_sequences , val_sequences = val_sequences , test_sequences = test_sequences , train_batch_size = parameters [ \"train_batch_size\" ], val_batch_size = parameters [ \"val_batch_size\" ], train_workers = parameters [ \"train_workers\" ], val_workers = parameters [ \"val_workers\" ], ) checkpoint_callback = callbacks . ModelCheckpoint ( dirpath = dir_path , save_top_k = 1 , verbose = True , monitor = \"valid/loss\" , mode = \"min\" , ) early_stopping_callback = callbacks . EarlyStopping ( monitor = \"valid/loss\" , patience = 2 , verbose = True , mode = \"min\" , ) trainer = Trainer ( max_epochs = parameters [ \"max_epochs\" ], logger = logger , callbacks = [ checkpoint_callback , early_stopping_callback ], gpus = gpu_value , log_every_n_steps = parameters [ \"log_n_steps\" ], progress_bar_refresh_rate = 10 , deterministic = True , ) trainer . fit ( model , data_module ) trainer . test ( model , data_module ) return { \"training_done\" : True }","title":"Trainingloop"}]}