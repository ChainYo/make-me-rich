{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard LSTM model with PyTorch Lightning.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        batch_size: int,\n",
    "        dropout_rate: float,\n",
    "        hidden_size: int,\n",
    "        number_of_features: int,\n",
    "        number_of_layers: int,\n",
    "        run_on_gpu: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_features = number_of_features\n",
    "        self.number_of_layers = number_of_layers\n",
    "        self.run_on_gpu = run_on_gpu\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_rate,\n",
    "            hidden_size=self.hidden_size,\n",
    "            input_size=self.n_features,\n",
    "            num_layers=self.number_of_layers,\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "\n",
    "        lstm_out = (batch_size, sequence_length, hidden_size)\n",
    "        \"\"\"\n",
    "        if self.run_on_gpu:\n",
    "            self.lstm.flatten_parameters()\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = hidden[-1]\n",
    "        return self.regressor(out)\n",
    "\n",
    "\n",
    "class PricePredictor(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Training model with PyTorch Lightning.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        batch_size: int,\n",
    "        dropout_rate: float,\n",
    "        hidden_size: int,\n",
    "        learning_rate: float,\n",
    "        number_of_features: int,\n",
    "        number_of_layers: int,\n",
    "        run_on_gpu: bool,\n",
    "        criterion: nn.Module = nn.MSELoss(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = LSTMRegressor(\n",
    "            batch_size, dropout_rate, hidden_size, number_of_features, number_of_layers, run_on_gpu,\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = criterion\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        output = self.model(x)\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels.unsqueeze(dim=1))\n",
    "            return loss, output\n",
    "        return output\n",
    "\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequences, labels = batch\n",
    "        loss, _ = self(sequences, labels)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequences, labels = batch\n",
    "        loss, _ = self(sequences, labels)\n",
    "        self.log(\"valid/loss\", loss, on_step=True, on_epoch=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        sequences, labels = batch\n",
    "        loss, _ = self(sequences, labels)\n",
    "        self.log(\"test/loss\", loss, on_step=True, on_epoch=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class CryptoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the LSTM model used by PyTorch Lightning.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequences: List[Tuple[pd.DataFrame, float]]):\n",
    "        self.sequences = sequences\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        sequence, label = self.sequences[index]\n",
    "        return (torch.Tensor(sequence.to_numpy()),torch.tensor(label).float())\n",
    "\n",
    "\n",
    "class LSTMDataLoader(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    Data loader for the LSTM model.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        train_sequences: List[Tuple[pd.DataFrame, float]],\n",
    "        val_sequences: List[Tuple[pd.DataFrame, float]],\n",
    "        test_sequences: List[Tuple[pd.DataFrame, float]],\n",
    "        train_batch_size: int,\n",
    "        val_batch_size: int,\n",
    "        train_workers: int = 2,\n",
    "        val_workers: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_sequences = train_sequences\n",
    "        self.val_sequences = val_sequences\n",
    "        self.test_sequences = test_sequences\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.train_workers = train_workers\n",
    "        self.val_workers = val_workers\n",
    "        self.test_workers = val_workers\n",
    "\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        \"\"\"\n",
    "        Load the data.\n",
    "        \"\"\"\n",
    "        self.train_dataset = CryptoDataset(self.train_sequences)\n",
    "        self.val_dataset = CryptoDataset(self.val_sequences)\n",
    "        self.test_dataset = CryptoDataset(self.test_sequences)\n",
    "\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.train_batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.train_workers\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.val_batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.val_workers\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset, \n",
    "            batch_size=self.val_batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=self.test_workers\n",
    "        )\n",
    "\n",
    "\n",
    "def create_sequences(\n",
    "    input_data: pd.DataFrame, \n",
    "    target_column: str, \n",
    "    sequence_length: int\n",
    "    ) -> List[Tuple[pd.DataFrame, float]]:\n",
    "    \"\"\"\n",
    "    Create sequences from the input data.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    size = len(input_data)\n",
    "    for i in range(size - sequence_length):\n",
    "        sequence = input_data[i: i + sequence_length]\n",
    "        label_position = i + sequence_length\n",
    "        label = input_data.iloc[label_position][target_column]\n",
    "        sequences.append([sequence, label])\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def split_train_and_val_sequences(\n",
    "    sequences: List[Tuple[pd.DataFrame, float]],\n",
    "    val_size: float,\n",
    ") -> Tuple[List[Tuple[pd.DataFrame, float]]]:\n",
    "    \"\"\"\n",
    "    Split sequences into training and validation sets.\n",
    "    \"\"\"\n",
    "    train_sequences, val_sequences = [], []\n",
    "    for sequence, label in sequences:\n",
    "        if len(train_sequences) < len(sequences) * (1 - val_size):\n",
    "            train_sequences.append((sequence, label))\n",
    "        else:\n",
    "            val_sequences.append((sequence, label))\n",
    "    return train_sequences, val_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/05_model_input/scaled_train_data.csv\")\n",
    "test_df = pd.read_csv(\"../data/05_model_input/scaled_test_data.csv\")\n",
    "\n",
    "test_sequences = create_sequences(test_df, \"close\", 60)\n",
    "train_sequences = create_sequences(train_df, \"close\", 60)\n",
    "train_sequences, val_sequences = split_train_and_val_sequences(train_sequences, 0.2)\n",
    "\n",
    "data = LSTMDataLoader(\n",
    "    train_sequences=train_sequences, \n",
    "    val_sequences=val_sequences, \n",
    "    test_sequences=test_sequences, \n",
    "    train_batch_size=2, \n",
    "    val_batch_size=1,\n",
    ")\n",
    "data.setup()\n",
    "model = PricePredictor.load_from_checkpoint(\"../data/06_models/epoch=4-step=349.ckpt\")\n",
    "X, y = next(iter(data.train_dataloader()))\n",
    "torch_out = model(X.to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CPUExecutionProvider']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_onnx_model = \"../data/06_models/model.onnx\"\n",
    "onnx_model = onnx.load(path_onnx_model)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(path_onnx_model)\n",
    "ort_session.get_providers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(X)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=0.001, atol=0.001\n\nMismatched elements: 2 / 2 (100%)\nMax absolute difference: 0.01724917\nMax relative difference: 0.01902613\n x: array([[-0.923853],\n       [-0.896494]], dtype=float32)\n y: array([[-0.906604],\n       [-0.90767 ]], dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32761/1774830265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mort_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All good!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/make-me-rich/lib/python3.8/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[1;32m    842\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nNot equal to tolerance rtol=0.001, atol=0.001\n\nMismatched elements: 2 / 2 (100%)\nMax absolute difference: 0.01724917\nMax relative difference: 0.01902613\n x: array([[-0.923853],\n       [-0.896494]], dtype=float32)\n y: array([[-0.906604],\n       [-0.90767 ]], dtype=float32)"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=0.001, atol=0.001)\n",
    "print(\"All good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8859],\n",
       "        [-0.9047]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.90660405],\n",
       "        [-0.9076697 ]], dtype=float32)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ce8d1aa01d11f3f70efbcde7bb935f26d292c4b979d96cad729611ab8905e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('make-me-rich': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
